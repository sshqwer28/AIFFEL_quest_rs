{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6860d982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p ./data\n",
    "# !wget https://s3.eu-central-1.amazonaws.com/avg-kitti/data_semantics.zip\n",
    "# !unzip data_semantics.zip -d ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58f18226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install albumentations scikit-image opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48b7a89",
   "metadata": {},
   "source": [
    "### 문제 정의(가정)\n",
    "1. epoch가 많으면 모델의 성능이 좋은가?(epoch을 모두 완료한 모델 기준)    \n",
    "    - epoch 50번 학습했을 때와 epoch 100번 학습했을 때 차이를 보면 epoch가 많아야 한다는 것은 분명해 보인다. 하지만 많다는 기준은 모호하고 정확한 기준을 세우기 어렵고 모델의 성능 지표에서 우선도가 낮다고 판단해 최소 100번의 학습을 다음 모델을 학습하고자 결정함  \n",
    "    [epoch50 img1](./img/Loss_Epoch/E50_003.png), [epoch50 img2](./img/Loss_Epoch/E50_029.png)    \n",
    "    [epoch100 img1](./img/Loss_Epoch/E100_003.png), [epoch100 img2](./Loss_Epoch/img/E100_029.png) \n",
    "\n",
    "2. Loss가 낮으면 모델의 성능이 좋은가?  \n",
    "    - UNetV2의 Validation 오버레이 결과를 비교해 봤을 때 대부분의 결과에서 Best Loss와 Last Loss가 비슷한 양상을 보이고 일부 결과에서 간간이 차이가 보이긴 하지만 이를 통해 Loss가 낮으면 모델의 성능 좋은가에 대한 결정을 내릴 수 없을 거 같다. \n",
    "    > 내가 봤을 땐 Best Loss가 조금 더 좋아보여서 이후 테스크는 Best Loss로 비교하도록 하겠다.  \n",
    "    [Best Loss img1](./img/Loss_Epoch/bestLoss_026.png) [Best Loss img2](./img/Loss_Epoch/bestLoss_002.png)  \n",
    "    [Last Loss img1](./img/Loss_Epoch/bestLoss_026.png) [Last Loss img2](./img/Loss_Epoch/bestLoss_002.png) \n",
    "\n",
    "3. 도로인지 아닌지의 테스크로 도로 영역을 구분하기 쉬운가? [UNetV0(BCE) UNetV1(FocalLoss)]   \n",
    "    - 도로 판별을 위한 BCELoss와 비도로 확신 영역의 학습 효율을 높이는 Focal Loss를 병행하여, 복잡한 환경에서도 도로 경계를 정밀하게 구분할 수 있는 세그멘테이션 모델을 구축했습니다.  \n",
    "    [BCE img1](./img/BCEFOCAL/train/bce1.png)  [Focal img1](./img/BCEFOCAL/train/focal1.png)   \n",
    "    [BCE img2](./img/BCEFOCAL/test/bce7.png)  [Focal img2](./img/BCEFOCAL/test/focal7.png)   \n",
    "    > Focal Loss의 장점이 Focal img2에서 확인 할 수 있다 도로가 확실히 아닌 영역(하늘)을 배제하고 학습할 수 있다는 장점이 있다.\n",
    "\n",
    "4. 기존 시맨틱 레이블의 유사 색상 배치로 인해 발생하는 도로-인도 간 경계 모호성을 해결하기 위해, semantic_rgb를 활용하여 클래스 간 대비를 극대화하는 가설을 설정하였습니다. 이를 통해 레이블의 특징 추출(Feature Extraction) 효율을 높이고, 경계 영역에서의 오분류를 최소화하는 강건한(Robust) 모델 생성을 목표로 합니다  \n",
    "    -  semantic_rgb를 활용한 모델(v2)가 대부분의 결과에서 나은 모습을 확인할 수 있었으나 아직도 도로의 경계선(사이드), 그림자, 장애물을 구별하지 못하는 것을 확인할 수 있다. \n",
    "    - UNet v2 img2를 보면 v1 모델의 성능이 낮은 것을 확인할 수 있는데 이것을 생각해보면 저 영역을 모델이 학습할 때 train 데이터와 증강 방법은 동일하기 때문에 레이블이 모호했거나 학습 과정(역전파)이 잘못되었다고 생각한다.  \n",
    "    [UNet v1 img1](./img/GRAYRGB/train/gray.png), [UNet v2 img1](./img/GRAYRGB/train/rgb6_FOCAL_FP.png)  \n",
    "    [UNet v1 img2](./img/GRAYRGB/test/g4.png), [UNet v2 img2](./img/GRAYRGB/train/r6.png)  \n",
    "    [두 모델에서 정확하지 않은 결과](./img/GRAYRGB/test/FP4.png)\n",
    "\n",
    "5. UNet과 UNet++의 성능지표로 어떤 것을 사용해야 하는가?\n",
    " - Validation은 IoU를 구할 수 있기 때문에 정량적인 평가가 가능하다 하지만, Test 데이터는 레이블이 없기 때문에 IoU를 구할 수 없다 따라서 정성적인 평가가 필요하다. \n",
    " - 이번 프로젝트에선 두 모델의 예측을 비교해 겹치지 않는 부분을 다른 색으로 표현 어떤 모델이 더 도로 영역을 잘 나타내는 가를 확인하고자 한다.  \n",
    " - UNet++의 연구를 많이 진행하지 못해서 UNet보다 더 좋은 가에 대한 답을 하기 어려울 거 같다. 여러 결과를 확인해보면 각 모델에서 잘 찾은 부분이나 못 찾은 부분이 많이 보였기 때문에\n",
    " 이러한 결과가 모델 구조, 데이터셋, 학습과정, 하이퍼파라미터 튜닝, 레이블, 증강, 다양한 기법 도입 등을 많은 요인이 있기에 어떤 모델이 더 나은가에 대한 답을 하기 어려울 거 같다.\n",
    " [UNet v2 img1](./img/UNetUNetPP/train/UN3.png), [UNet++ img1](./img/UNetUNetPP/train/UNP1.png)  \n",
    " [UNet v2 img2](./img/UNetUNetPP/test/UN3.png), [UNet++ img2](./img/UNetUNetPP/train/UNP6.png)  \n",
    " [두 모델에서 정확하지 않은 결과](./img/UNetUNetPP/test/FP1.png)\n",
    "\n",
    "### 회고\n",
    "아쉽다.... 여기서 더 나아가서 도로 영역(레이블)에서 모델의 예측을 빼고 모델을 추가적으로 학습하는 ACoL을 구현하고 싶었지만 시간 부족해서 구현하지 못한 것이 너무 아쉽고, 도로 영역만 구별하는 테스크(한 개만 찾는 것)가 더 어려울 지 아니면 여러 객체를 동시에 구별하는 테스크(멀티 클래스)가 더 어려울 지 잘 모르겠다...\n",
    "내 생각은 멀티 클래스가 더 쉬울 것으로 생각되긴 하지만 어렵다. 이번 프로젝트도 시간이 너무 부족하고 해야할 건 너무 많고 프로젝트도 어렵다... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7215c1d",
   "metadata": {},
   "source": [
    "### 시맨틱 세그멘테이션 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ffa0928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "#필요한 라이브러리를 로드합니다.\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from albumentations import (\n",
    "    Compose, HorizontalFlip, Affine, Resize, \n",
    "    CropNonEmptyMaskIfExists, RandomShadow, RandomBrightnessContrast, \n",
    "    RandomGamma, OneOf, GaussNoise, MultiplicativeNoise, RandomSizedCrop, ShiftScaleRotate\n",
    ")\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59da169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 100\n",
    "batch_size = 8\n",
    "data_dir = './data/training'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 모델 저장 경로\n",
    "save_dir = './models'\n",
    "\n",
    "if save_dir: \n",
    "    os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3945c6e6",
   "metadata": {},
   "source": [
    "#### 증강에 따라 모델의 성능이 달라지는가(도로의 영역을 잘 찾는가?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58e7df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_augmentation(is_train=True):\n",
    "#     img_interp = cv2.INTER_LANCZOS4 \n",
    "    \n",
    "#     if is_train:\n",
    "#         return Compose([\n",
    "#             HorizontalFlip(p=0.5),\n",
    "#             RandomSizedCrop(\n",
    "#                 min_max_height=(300, 370),\n",
    "#                 w2h_ratio=0.5, # 도로 형체 보존을 위해 0.5 권장\n",
    "#                 size=(224, 224),\n",
    "#                 p=0.3,\n",
    "#                 interpolation=img_interp,           # 이미지: 고화질 보간\n",
    "#                 mask_interpolation=cv2.INTER_NEAREST # 마스크: 절대 값 유지\n",
    "#             ),\n",
    "#             Resize(\n",
    "#                 width=224,\n",
    "#                 height=224,\n",
    "#                 interpolation=img_interp,\n",
    "#                 mask_interpolation=cv2.INTER_NEAREST # 필수\n",
    "#             )\n",
    "#         ])\n",
    "#     return Compose([\n",
    "#         Resize(\n",
    "#             width=224,\n",
    "#             height=224,\n",
    "#             interpolation=img_interp,\n",
    "#             mask_interpolation=cv2.INTER_NEAREST # 필수\n",
    "#         )\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdb15dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_augmentation_v2(is_train=True):\n",
    "    img_interp = cv2.INTER_LANCZOS4 \n",
    "    \n",
    "    if is_train:\n",
    "        return Compose([\n",
    "            # 1. 도로 중심 크롭 \n",
    "            CropNonEmptyMaskIfExists(\n",
    "                height=370, \n",
    "                width=740, \n",
    "                p=1.0\n",
    "            ),\n",
    "\n",
    "            # 2. 기하학적 변형 (Affine 최신 표준)\n",
    "            HorizontalFlip(p=0.5),\n",
    "            Affine(\n",
    "                translate_percent={\"x\": (-0.03, 0.03), \"y\": (-0.03, 0.03)}, \n",
    "                scale=(0.95, 1.05), \n",
    "                rotate=(-3, 3),\n",
    "                border_mode=cv2.BORDER_REPLICATE, \n",
    "                p=0.4\n",
    "            ),\n",
    "\n",
    "            # 3. 환경 대응 \n",
    "            OneOf([\n",
    "                RandomShadow(num_shadows_limit=(1, 2), shadow_dimension=5, p=1.0),\n",
    "                RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=1.0),\n",
    "                RandomGamma(gamma_limit=(80, 120), p=1.0),\n",
    "            ], p=0.6),\n",
    "\n",
    "            # 4. 노이즈\n",
    "            GaussNoise(std_range=(0.0, 0.01), p=0.2),\n",
    "\n",
    "            Resize(width=224, height=224, interpolation=img_interp, mask_interpolation=cv2.INTER_NEAREST)\n",
    "        ])\n",
    "    \n",
    "    return Compose([\n",
    "        Resize(width=224, height=224, interpolation=img_interp, mask_interpolation=cv2.INTER_NEAREST)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "782ab8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_augmentation_v3(is_train=True):\n",
    "#     img_interp = cv2.INTER_LANCZOS4 \n",
    "#     if is_train:\n",
    "#         return Compose([\n",
    "#             HorizontalFlip(p=0.5),\n",
    "#             # 과한 Crop 대신, 안정적인 위치 변화를 줍니다.\n",
    "#             ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=5, p=0.5),\n",
    "#             # 조명 조건 대응\n",
    "#             RandomBrightnessContrast(p=0.5),\n",
    "#             # 최종 사이즈 결정 (왜곡은 여기서 어쩔 수 없이 발생하지만, 앞단에서 최소화함)\n",
    "#             Resize(width=224, height=224,\n",
    "#                 interpolation=img_interp,\n",
    "#                 mask_interpolation=cv2.INTER_NEAREST # 필수\n",
    "#                 )\n",
    "#         ])\n",
    "#     return Compose([\n",
    "#         Resize(width=224, height=224,\n",
    "#                 interpolation=img_interp,\n",
    "#                 mask_interpolation=cv2.INTER_NEAREST # 필수\n",
    "#                 )\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf122b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 증강이 적절하게 되는가(확인용)\n",
    "# augmentation_train =build_augmentation_v2()\n",
    "# augmentation_test = build_augmentation_v2(is_train=False)\n",
    "# # 1. 마스크 파일 경로 목록 생성 (image_2 폴더를 semantic 폴더로 변경)\n",
    "# # 데이터셋 구조에 따라 경로를 수정하세요. (예: image_2 -> semantic)\n",
    "# mask_images = sorted(glob(os.path.join(data_dir, \"semantic\", \"*.png\")))\n",
    "# input_images = sorted(glob(os.path.join(data_dir, \"image_2\", \"*.png\")))\n",
    "\n",
    "# plt.figure(figsize=(10, 15))\n",
    "\n",
    "# for i in range(5):\n",
    "#     # 이미지와 마스크 로드\n",
    "#     image = imread(input_images[i])\n",
    "#     mask = imread(mask_images[i]) # 해당 이미지의 정답 마스크 로드\n",
    "    \n",
    "#     # [핵심] image와 mask를 동시에 딕셔너리에 담기\n",
    "#     # CropNonEmptyMaskIfExists가 이 mask를 보고 도로 영역을 찾아냅니다.\n",
    "#     image_data = {\"image\": image, \"mask\": mask}\n",
    "    \n",
    "#     # 2. 증강 적용\n",
    "#     # Test용 (단순 Resize)\n",
    "#     resized = augmentation_test(**image_data)\n",
    "#     # Train용 (도로 중심 크롭 + 그림자 등 적용)\n",
    "#     processed = augmentation_train(**image_data)\n",
    "    \n",
    "#     # --- 시각화 ---\n",
    "#     # 왼쪽: 단순 Resize 결과\n",
    "#     plt.subplot(5, 2, 2*i+1)\n",
    "#     plt.imshow(resized[\"image\"])\n",
    "#     plt.title(f\"Original (Resized) {i+1}\")\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#     # 오른쪽: 도로 중심 크롭 및 증강 결과\n",
    "#     plt.subplot(5, 2, 2*i+2)\n",
    "#     plt.imshow(processed[\"image\"])\n",
    "#     plt.title(f\"Road-Centered Aug {i+1}\")\n",
    "#     plt.axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8314b0",
   "metadata": {},
   "source": [
    "---\n",
    "#### KittiDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85b2d7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KittiDataset(Dataset):\n",
    "    '''\n",
    "    KittiDataset은 PyTorch의 Dataset을 상속받습니다.\n",
    "    우리가 KittiDataset을 원하는 방식으로 preprocess하기 위해서 Dataset을 커스텀하여 사용합니다.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 dir_path,\n",
    "                 img_size=(224, 224, 3),\n",
    "                 output_size=(224, 224),\n",
    "                 is_train=True,\n",
    "                 augmentation=None):\n",
    "        '''\n",
    "        dir_path: dataset의 directory path입니다.\n",
    "        img_size: preprocess에 사용할 입력이미지의 크기입니다.\n",
    "        output_size: ground_truth를 만들어주기 위한 크기입니다.\n",
    "        is_train: 이 Dataset이 학습용인지 테스트용인지 구분합니다.\n",
    "        augmentation: 적용하길 원하는 augmentation 함수를 인자로 받습니다.\n",
    "        '''\n",
    "        self.dir_path = dir_path\n",
    "        self.is_train = is_train\n",
    "        self.augmentation = augmentation\n",
    "        self.img_size = img_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # load_dataset()을 통해 kitti dataset의 경로에서 라벨과 이미지를 확인합니다.\n",
    "        self.data = self.load_dataset()\n",
    "\n",
    "    def load_dataset(self):\n",
    "        # kitti dataset에서 필요한 정보(이미지 경로 및 라벨)를 directory에서 확인하고 로드하는 함수입니다.\n",
    "        input_images = sorted(glob(os.path.join(self.dir_path, \"image_2\", \"*.png\")))\n",
    "        label_images = sorted(glob(os.path.join(self.dir_path, \"semantic\", \"*.png\")))\n",
    "\n",
    "        assert len(input_images) == len(label_images)\n",
    "        data = list(zip(input_images, label_images))\n",
    "\n",
    "        if self.is_train:\n",
    "            return data[:-40]\n",
    "        return data[-40:]\n",
    "\n",
    "    def __len__(self):\n",
    "        # Dataset의 length로서 전체 dataset 크기를 반환합니다.\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 입력과 출력을 만듭니다.\n",
    "        # 입력은 resize 및 augmentation이 적용된 input image이고\n",
    "        # 출력은 semantic label입니다.\n",
    "        input_img_path, output_path = self.data[index]\n",
    "\n",
    "        _input = cv2.cvtColor(imread(input_img_path), cv2.COLOR_BGR2RGB)\n",
    "        _output = imread(output_path)\n",
    "\n",
    "        # 특정 라벨을 이진 마스크로 변환(중요)\n",
    "        _output = (_output == 7).astype(np.uint8) * 1\n",
    "\n",
    "        data = {\n",
    "            \"image\": _input,\n",
    "            \"mask\": _output,\n",
    "        }\n",
    "\n",
    "        if self.augmentation:\n",
    "            augmented = self.augmentation(**data)\n",
    "            _input = augmented[\"image\"]  \n",
    "            _output = augmented[\"mask\"]\n",
    "\n",
    "        if _input.max() > 1.0: \n",
    "            _input = _input / 255.0 # Normalize\n",
    "\n",
    "        # target 차원 확장 (H, W) -> (1, H, W)\n",
    "        # Albumentations를 거치면 차원이 사라질 수 있으므로 마지막에 하는 것이 안전\n",
    "        if _output.ndim == 2:\n",
    "            _output = np.expand_dims(_output, axis=0)\n",
    "\n",
    "        return (\n",
    "            torch.tensor(_input, dtype=torch.float32).permute(2, 0, 1),  # (H, W, C) → (C, H, W)\n",
    "            torch.tensor(_output, dtype=torch.float32)  # (1, H, W) 형식 유지\n",
    "        )\n",
    "\n",
    "    def shuffle_data(self):\n",
    "        # 한 epoch가 끝나면 실행되는 함수입니다. 학습 중인 경우에 데이터를 random shuffle합니다.\n",
    "        if self.is_train:\n",
    "            np.random.shuffle(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1939348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic_rgb에서 도로 영역(1) 외 나머지 객체를 0으로 처리\n",
    "class KittiDatasetV2(Dataset):\n",
    "    def __init__(self,\n",
    "                 dir_path,\n",
    "                 img_size=(224, 224, 3),\n",
    "                 output_size=(224, 224),\n",
    "                 is_train=True,\n",
    "                 augmentation=None):\n",
    "        self.dir_path = dir_path\n",
    "        self.is_train = is_train\n",
    "        self.augmentation = augmentation\n",
    "        self.img_size = img_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # 데이터 경로 로드\n",
    "        self.data = self.load_dataset()\n",
    "\n",
    "    def load_dataset(self):\n",
    "        # image_2 폴더와 semantic 폴더에서 파일 리스트를 가져옵니다.\n",
    "        input_images = sorted(glob(os.path.join(self.dir_path, \"image_2\", \"*.png\")))\n",
    "        label_images = sorted(glob(os.path.join(self.dir_path, \"semantic_rgb\", \"*.png\")))\n",
    "\n",
    "        assert len(input_images) == len(label_images), \"이미지와 라벨의 개수가 일치하지 않습니다.\"\n",
    "        data = list(zip(input_images, label_images))\n",
    "\n",
    "        if self.is_train:\n",
    "            return data[:-40]\n",
    "        return data[-40:]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_img_path, output_path = self.data[index]\n",
    "\n",
    "        # 1. 이미지 및 RGB 라벨 로드\n",
    "        # imread 대신 cv2를 사용하여 색상 채널을 명확히 관리합니다.\n",
    "        _input = cv2.imread(input_img_path)\n",
    "        _input = cv2.cvtColor(_input, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        _output_rgb = cv2.imread(output_path)\n",
    "        _output_rgb = cv2.cvtColor(_output_rgb, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # 2. [Semantic v2 핵심] RGB 색상 매칭을 통한 이진 마스크 생성\n",
    "        # KITTI/Cityscapes 표준 도로 색상: 보라색 [128, 64, 128]\n",
    "        # 사용자님의 데이터에 맞춰 이 값을 수정할 수 있습니다.\n",
    "        road_color = [128, 64, 128] \n",
    "        _output = np.all(_output_rgb == road_color, axis=-1).astype(np.uint8)\n",
    "\n",
    "        data = {\n",
    "            \"image\": _input,\n",
    "            \"mask\": _output,\n",
    "        }\n",
    "\n",
    "        # 3. Augmentation 적용\n",
    "        if self.augmentation:\n",
    "            augmented = self.augmentation(**data)\n",
    "            _input = augmented[\"image\"]  \n",
    "            _output = augmented[\"mask\"]\n",
    "\n",
    "        # 4. 정규화 (Normalization)\n",
    "        if _input.max() > 1.0: \n",
    "            _input = _input / 255.0\n",
    "\n",
    "        # 5. 차원 맞추기 (H, W) -> (1, H, W)\n",
    "        if _output.ndim == 2:\n",
    "            _output = np.expand_dims(_output, axis=0)\n",
    "\n",
    "        # 6. Tensor 변환 및 채널 순서 변경 (H,W,C -> C,H,W)\n",
    "        return (\n",
    "            torch.tensor(_input, dtype=torch.float32).permute(2, 0, 1),\n",
    "            torch.tensor(_output, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    def shuffle_data(self):\n",
    "        if self.is_train:\n",
    "            np.random.shuffle(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ce4a687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation 설정s\n",
    "augmentation = build_augmentation_v2()\n",
    "val_preproc = build_augmentation_v2(is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63ade4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = KittiDataset(\n",
    "    data_dir,\n",
    "    augmentation=augmentation,\n",
    "    is_train=True\n",
    ")\n",
    "\n",
    "val_dataset = KittiDataset(\n",
    "    data_dir,\n",
    "    augmentation=val_preproc,\n",
    "    is_train=False\n",
    ")\n",
    "\n",
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d4e9b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_v2 = KittiDatasetV2(\n",
    "    data_dir,\n",
    "    augmentation=augmentation,\n",
    "    is_train=True\n",
    ")\n",
    "\n",
    "val_dataset_v2 = KittiDatasetV2(\n",
    "    data_dir,\n",
    "    augmentation=val_preproc,\n",
    "    is_train=False\n",
    ")\n",
    "\n",
    "# DataLoader 설정\n",
    "train_loader_v2 = DataLoader(train_dataset_v2, batch_size=batch_size, shuffle=True)\n",
    "val_loader_v2 = DataLoader(val_dataset_v2, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54d45d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V1 첫번째 이미지: ./data/training/image_2/000165_10.png\n",
      "V2 첫번째 이미지: ./data/training/image_2/000165_10.png\n",
      "두 데이터셋의 이미지가 완전히 같은가? : True\n"
     ]
    }
   ],
   "source": [
    "# 두 데이터셋의 검증용 데이터 경로를 가져와 비교\n",
    "val_images = [d[0] for d in val_dataset.data] # KittiDataset (is_train=False일 때)\n",
    "val_images_v2 = [d[0] for d in val_dataset_v2.data] # KittiDatasetV2 (is_train=False일 때)\n",
    "\n",
    "# 첫 번째 파일명이 같은지 확인\n",
    "print(f\"V1 첫번째 이미지: {val_images[5]}\")\n",
    "print(f\"V2 첫번째 이미지: {val_images_v2[5]}\")\n",
    "\n",
    "# 전체가 완벽히 일치하는지 확인\n",
    "print(f\"두 데이터셋의 이미지가 완전히 같은가? : {val_images == val_images_v2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b79393c",
   "metadata": {},
   "source": [
    "### 시맨틱 세그멘테이션 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b6f9bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet_block(nn.Module):\n",
    "    def __init__(self, in_channels, mid_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(mid_channels)\n",
    "        self.conv2 = nn.Conv2d(mid_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb210d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, input_channels=3, output_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # [Encoder] - Unet_block 사용\n",
    "        self.enc1 = Unet_block(input_channels, 64, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = Unet_block(64, 128, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = Unet_block(128, 256, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.enc4 = Unet_block(256, 512, 512)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        # [Bottleneck]\n",
    "        self.bottleneck = Unet_block(512, 1024, 1024)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # [Decoder]\n",
    "        self.up6 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.dec6 = Unet_block(1024, 512, 512) # Cat(512+512) -> 512\n",
    "        self.up7 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec7 = Unet_block(512, 256, 256)\n",
    "        self.up8 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec8 = Unet_block(256, 128, 128)\n",
    "        self.up9 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec9 = Unet_block(128, 64, 64)\n",
    "\n",
    "        self.final = nn.Conv2d(64, output_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        c1 = self.enc1(x)\n",
    "        p1 = self.pool1(c1)\n",
    "        c2 = self.enc2(p1)\n",
    "        p2 = self.pool2(c2)\n",
    "        c3 = self.enc3(p2)\n",
    "        p3 = self.pool3(c3)\n",
    "        c4 = self.enc4(p3)\n",
    "        p4 = self.pool4(c4)\n",
    "\n",
    "        # Bottleneck\n",
    "        c5 = self.bottleneck(p4)\n",
    "        c5 = self.dropout(c5)\n",
    "\n",
    "        # Decoder\n",
    "        u6 = self.up6(c5)\n",
    "        u6 = torch.cat([u6, c4], dim=1)\n",
    "        c6 = self.dec6(u6)\n",
    "\n",
    "        u7 = self.up7(c6)\n",
    "        u7 = torch.cat([u7, c3], dim=1)\n",
    "        c7 = self.dec7(u7)\n",
    "\n",
    "        u8 = self.up8(c7)\n",
    "        u8 = torch.cat([u8, c2], dim=1)\n",
    "        c8 = self.dec8(u8)\n",
    "\n",
    "        u9 = self.up9(c8)\n",
    "        u9 = torch.cat([u9, c1], dim=1)\n",
    "        c9 = self.dec9(u9)\n",
    "\n",
    "        output = self.final(c9)\n",
    "        return torch.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "550af0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        init.constant_(m.weight, 1)\n",
    "        init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "700febbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Nested_UNet(nn.Module):\n",
    "    def __init__(self, input_channels=3, output_channels=1, deep_supervision=False):\n",
    "        super().__init__()\n",
    "\n",
    "        # 모델 크기 설정 (원할 경우 [64, 128, 256, 512, 1024]로 변경 가능)\n",
    "        nb_filter = [64, 128, 256, 512, 1024]\n",
    "        \n",
    "        self.deep_supervision = deep_supervision\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        # --- Backbone (DownSampling) ---\n",
    "        self.conv0_0 = Unet_block(input_channels, nb_filter[0], nb_filter[0])\n",
    "        self.conv1_0 = Unet_block(nb_filter[0], nb_filter[1], nb_filter[1])\n",
    "        self.conv2_0 = Unet_block(nb_filter[1], nb_filter[2], nb_filter[2])\n",
    "        self.conv3_0 = Unet_block(nb_filter[2], nb_filter[3], nb_filter[3])\n",
    "        self.conv4_0 = Unet_block(nb_filter[3], nb_filter[4], nb_filter[4])\n",
    "\n",
    "        # --- Upsampling & Dense skip connections ---\n",
    "        # N to 1 skip (X^0,1 | X^1,1 | X^2,1 | X^3,1)\n",
    "        self.conv0_1 = Unet_block(nb_filter[0] + nb_filter[1], nb_filter[0], nb_filter[0])\n",
    "        self.conv1_1 = Unet_block(nb_filter[1] + nb_filter[2], nb_filter[1], nb_filter[1])\n",
    "        self.conv2_1 = Unet_block(nb_filter[2] + nb_filter[3], nb_filter[2], nb_filter[2])\n",
    "        self.conv3_1 = Unet_block(nb_filter[3] + nb_filter[4], nb_filter[3], nb_filter[3])\n",
    "       \n",
    "        # N to 2 skip (X^0,2 | X^1,2 | X^2,2)\n",
    "        self.conv0_2 = Unet_block(nb_filter[0]*2 + nb_filter[1], nb_filter[0], nb_filter[0])\n",
    "        self.conv1_2 = Unet_block(nb_filter[1]*2 + nb_filter[2], nb_filter[1], nb_filter[1])\n",
    "        self.conv2_2 = Unet_block(nb_filter[2]*2 + nb_filter[3], nb_filter[2], nb_filter[2])\n",
    "\n",
    "        # N to 3 skip (X^0,3 | X^1,3)\n",
    "        self.conv0_3 = Unet_block(nb_filter[0]*3 + nb_filter[1], nb_filter[0], nb_filter[0])\n",
    "        self.conv1_3 = Unet_block(nb_filter[1]*3 + nb_filter[2], nb_filter[1], nb_filter[1])\n",
    "\n",
    "        # N to 4 skip (X^0,4)\n",
    "        self.conv0_4 = Unet_block(nb_filter[0]*4 + nb_filter[1], nb_filter[0], nb_filter[0])\n",
    "\n",
    "        # --- Output Layers ---\n",
    "        if self.deep_supervision:\n",
    "            self.output1 = nn.Conv2d(nb_filter[0], output_channels, kernel_size=1)\n",
    "            self.output2 = nn.Conv2d(nb_filter[0], output_channels, kernel_size=1)\n",
    "            self.output3 = nn.Conv2d(nb_filter[0], output_channels, kernel_size=1)\n",
    "            self.output4 = nn.Conv2d(nb_filter[0], output_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.output = nn.Conv2d(nb_filter[0], output_channels, kernel_size=1)\n",
    "\n",
    "        # 모델 파라미터 Kaiming 초기화 적용\n",
    "        self.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 대각선(Diagonal) 흐름으로 계산하여 메모리 할당 효율성 극대화\n",
    "        \n",
    "        # Level 0 & 1\n",
    "        x0_0 = self.conv0_0(x)               \n",
    "        x1_0 = self.conv1_0(self.pool(x0_0))\n",
    "        x0_1 = self.conv0_1(torch.cat([x0_0, self.up(x1_0)], dim=1))\n",
    "        \n",
    "        # Level 2\n",
    "        x2_0 = self.conv2_0(self.pool(x1_0))\n",
    "        x1_1 = self.conv1_1(torch.cat([x1_0, self.up(x2_0)], dim=1))\n",
    "        x0_2 = self.conv0_2(torch.cat([x0_0, x0_1, self.up(x1_1)], dim=1))\n",
    "\n",
    "        # Level 3\n",
    "        x3_0 = self.conv3_0(self.pool(x2_0))\n",
    "        x2_1 = self.conv2_1(torch.cat([x2_0, self.up(x3_0)], dim=1))\n",
    "        x1_2 = self.conv1_2(torch.cat([x1_0, x1_1, self.up(x2_1)], dim=1))\n",
    "        x0_3 = self.conv0_3(torch.cat([x0_0, x0_1, x0_2, self.up(x1_2)], dim=1))\n",
    "\n",
    "        # Level 4\n",
    "        x4_0 = self.conv4_0(self.pool(x3_0))\n",
    "        x3_1 = self.conv3_1(torch.cat([x3_0, self.up(x4_0)], dim=1))\n",
    "        x2_2 = self.conv2_2(torch.cat([x2_0, x2_1, self.up(x3_1)], dim=1))\n",
    "        x1_3 = self.conv1_3(torch.cat([x1_0, x1_1, x1_2, self.up(x2_2)], dim=1))\n",
    "        x0_4 = self.conv0_4(torch.cat([x0_0, x0_1, x0_2, x0_3, self.up(x1_3)], dim=1))\n",
    "\n",
    "        # --- Return (Raw Logits 반환) ---\n",
    "        if self.deep_supervision:\n",
    "            output1 = self.output1(x0_1)\n",
    "            output2 = self.output2(x0_2)\n",
    "            output3 = self.output3(x0_3)\n",
    "            output4 = self.output4(x0_4)\n",
    "            # 리스트 형태로 개별 Logit을 반환 (Loss 계산 시 각각 더해줌)\n",
    "            return [torch.sigmoid(output1), \n",
    "                    torch.sigmoid(output2), \n",
    "                    torch.sigmoid(output3), \n",
    "                    torch.sigmoid(output4)]\n",
    "        else:\n",
    "            # 최종 아웃풋 Logit만 반환\n",
    "            output = self.output(x0_4)\n",
    "            return torch.sigmoid(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d073129",
   "metadata": {},
   "source": [
    "### Train Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "155b6cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(pred, target, threshold=0.5):\n",
    "    # pred: (B, 1, H, W), target: (B, 1, H, W)\n",
    "    pred = (torch.sigmoid(pred) > threshold).float()\n",
    "    target = target.float()\n",
    "\n",
    "    # 각 이미지별로 Intersection과 Union 계산 (dim 1, 2, 3에 대해 합산)\n",
    "    # 결과는 (Batch_size,) 형태의 벡터가 됨\n",
    "    intersection = (pred * target).sum(dim=(1, 2, 3))\n",
    "    union = pred.sum(dim=(1, 2, 3)) + target.sum(dim=(1, 2, 3)) - intersection\n",
    "\n",
    "    # 이미지별 IoU 계산 (0으로 나누기 방지)\n",
    "    iou_per_image = (intersection + 1e-6) / (union + 1e-6)\n",
    "    \n",
    "    # 이미지별 Pixel Accuracy 계산\n",
    "    acc_per_image = (pred == target).float().mean(dim=(1, 2, 3))\n",
    "    \n",
    "    # 배치 평균값 반환\n",
    "    return iou_per_image.mean().item(), acc_per_image.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "171dbec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, gamma=2.0, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # 1. BCE Loss 계산 (Logits을 입력으로 받음)\n",
    "        # targets는 [B, 1, H, W] 형태이며 float 타입이어야 함\n",
    "        bce_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n",
    "        \n",
    "        # 2. pt (정답에 대한 확률) 계산\n",
    "        pt = torch.exp(-bce_loss) \n",
    "        \n",
    "        # 3. Focal Loss 공식 적용: FL(pt) = -alpha * (1 - pt)^gamma * log(pt)\n",
    "        focal_loss = self.alpha * (1 - pt)**self.gamma * bce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f22170fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, device, num_epochs, save_dir, lr=1e-4,model_type='_unet'):\n",
    "    best_loss = float('inf')\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
    "\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    # scheduler 정의 (patience=5는 5에폭 동안 개선 없을 시 LR 감소)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=1e-3,              # 도달하고 싶은 최대 학습률 (보통 초기 lr의 10배 정도)\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=num_epochs,\n",
    "        pct_start=0.3,            # 전체 학습 중 30% 구간을 워밍업(학습률 상승)에 할당\n",
    "        anneal_strategy='cos',    # 코사인 곡선으로 학습률 하강\n",
    "        div_factor=25.0,          # 시작 학습률 = max_lr / 25.0\n",
    "        final_div_factor=10000.0  # 최종 학습률 = 시작 학습률 / 10000.0 (거의 0에 가깝게)\n",
    "    )\n",
    "\n",
    "    # 1. 에폭 단위 진행바\n",
    "    epoch_pbar = tqdm(range(num_epochs), desc=\"Total Epochs\")\n",
    "\n",
    "    for epoch in epoch_pbar:\n",
    "        # --- [Training Phase] ---\n",
    "        model.train()\n",
    "        train_loss, train_iou, train_acc = 0.0, 0.0, 0.0\n",
    "\n",
    "        if hasattr(train_loader.dataset, 'shuffle_data'):\n",
    "            train_loader.dataset.shuffle_data()\n",
    "\n",
    "        # 2. 학습 배치 단위 진행바 (leave=False로 에폭 종료 시 정리)\n",
    "        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\", leave=False)\n",
    "        for inputs, targets in train_pbar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            if isinstance(outputs, list):\n",
    "                loss = sum([criterion(o, targets.float()) for o in outputs])\n",
    "                final_output = outputs[-1] \n",
    "            else:\n",
    "                loss = criterion(outputs, targets.float())\n",
    "                final_output = outputs\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 학습률 스케줄러 업데이트\n",
    "            scheduler.step()\n",
    "\n",
    "            # 지표 계산 및 업데이트\n",
    "            running_iou, running_acc = calculate_metrics(final_output, targets)\n",
    "            train_loss += loss.item()\n",
    "            train_iou += running_iou\n",
    "            train_acc += running_acc\n",
    "            \n",
    "            # 실시간 배치 정보 표시\n",
    "            train_pbar.set_postfix(loss=f\"{loss.item():.4f}\", iou=f\"{running_iou:.4f}\")\n",
    "\n",
    "        # --- [Validation Phase] ---\n",
    "        model.eval()\n",
    "        val_loss, val_iou, val_acc = 0.0, 0.0, 0.0\n",
    "\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\", leave=False)\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_pbar:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                if isinstance(outputs, list):\n",
    "                    v_loss = sum([criterion(o, targets.float()) for o in outputs])\n",
    "                    final_output = outputs[-1]\n",
    "                else:\n",
    "                    v_loss = criterion(outputs, targets.float())\n",
    "                    final_output = outputs\n",
    "                \n",
    "                v_iou, v_acc = calculate_metrics(final_output, targets)\n",
    "                val_loss += v_loss.item()\n",
    "                val_iou += v_iou\n",
    "                val_acc += v_acc\n",
    "                \n",
    "                val_pbar.set_postfix(loss=f\"{v_loss.item():.4f}\", iou=f\"{v_iou:.4f}\")\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else 0\n",
    "        avg_val_iou = val_iou / len(val_loader) if len(val_loader) > 0 else 0\n",
    "\n",
    "        # 에폭바에 최종 결과 출력\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        epoch_pbar.set_postfix(v_loss=f\"{avg_val_loss:.4f}\", v_iou=f\"{avg_val_iou:.4f}\", lr=f\"{current_lr:.1e}\")\n",
    "\n",
    "        # Best Model Save \n",
    "        if avg_val_loss < best_loss and avg_val_loss > 0:\n",
    "            best_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), f'{save_dir}/best{model_type}.pth')\n",
    "\n",
    "    torch.save(model.state_dict(), f'{save_dir}/last{model_type}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d3375d",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8afd75e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou_score(target, prediction, verbose=False):\n",
    "    if target.shape != prediction.shape:\n",
    "        prediction = resize(prediction, target.shape, mode='constant', preserve_range=True).astype(np.uint8)\n",
    "\n",
    "    intersection = np.logical_and(target, prediction).sum()\n",
    "    union = np.logical_or(target, prediction).sum()\n",
    "    iou_score = intersection / (union + 1e-7)  \n",
    "    if verbose: \n",
    "        print(f\"IoU : {iou_score:.6f}\")\n",
    "    return iou_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c65e1c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output(model, preproc, image_path, output_path, label_path = None, im_show = False):\n",
    "    # 1. 모델이 현재 위치한 장치를 자동으로 확인\n",
    "    # 모델 가중치의 첫 번째 파라미터를 기준으로 삼습니다.\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # 2. 이미지 로드 및 전처리 (사용자 기존 로직)\n",
    "    origin_img = imread(image_path)\n",
    "    data = {\"image\": origin_img}\n",
    "    processed = preproc(**data)\n",
    "    \n",
    "    # 3. 텐서 생성 및 장치 이동 (핵심 수정 포인트)\n",
    "    # / 255.0으로 정규화한 뒤, 모델과 같은 장치(device)로 보냅니다.\n",
    "    input_tensor = torch.tensor(processed[\"image\"] / 255.0, dtype=torch.float32)\n",
    "    input_tensor = input_tensor.permute(2, 0, 1).unsqueeze(0).to(device) # .to(device) 추가\n",
    "\n",
    "    # 4. 모델 추론\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "\n",
    "    # 5. 후처리 (시각화를 위해 다시 CPU로 가져옴)\n",
    "    # GPU에 있는 결과를 다시 넘파이로 바꾸려면 .cpu()가 필수입니다.\n",
    "    prediction = (output[0].squeeze().cpu().numpy() > 0.5).astype(np.uint8) * 255\n",
    "    prediction = Image.fromarray(prediction).convert('L')\n",
    "\n",
    "    # 6. 블렌딩 및 시각화 (사용자 기존 로직 유지)\n",
    "    background = Image.fromarray(origin_img).convert('RGBA')\n",
    "    prediction_resized = prediction.resize((origin_img.shape[1], origin_img.shape[0])).convert('RGBA')\n",
    "    blended = Image.blend(background, prediction_resized, alpha=0.5)\n",
    "\n",
    "    blended.save(output_path)\n",
    "    img_np = np.array(blended)\n",
    "\n",
    "    if im_show:\n",
    "        plt.imshow(img_np)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "    target = None\n",
    "    if label_path:\n",
    "        label_img = imread(label_path)\n",
    "        label_processed = preproc(image=label_img)[\"image\"]\n",
    "        target = (label_processed == 7).astype(np.uint8) * 1\n",
    "\n",
    "    return blended, np.array(prediction), target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b68ffe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comparison_output(model_u, model_pp, preproc, image_path, output_path, model_type, label_path=None, idx=0):\n",
    "    # 1. 모델 장치 확인\n",
    "    device_u = next(model_u.parameters()).device\n",
    "    device_pp = next(model_pp.parameters()).device\n",
    "    \n",
    "    # 2. 이미지 로드 및 채널 처리\n",
    "    origin_img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "    if origin_img is None: return\n",
    "\n",
    "    # 채널 수 확인 (1채널 vs 3채널)\n",
    "    if origin_img.ndim == 2:\n",
    "        h, w = origin_img.shape\n",
    "        c = 1\n",
    "        # 시각화용 배경은 3채널로 변환\n",
    "        vis_background = cv2.cvtColor(origin_img, cv2.COLOR_GRAY2RGB)\n",
    "    else:\n",
    "        h, w, c = origin_img.shape\n",
    "        vis_background = cv2.cvtColor(origin_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # 3. 모델 입력을 위한 전처리\n",
    "    # Dataset 로직: 입력은 RGB여야 함\n",
    "    img_rgb = cv2.cvtColor(origin_img, cv2.COLOR_BGR2RGB) if c == 3 else cv2.cvtColor(origin_img, cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    processed = preproc(image=img_rgb)\n",
    "    img_pre = processed[\"image\"]\n",
    "    \n",
    "    # 정규화\n",
    "    if img_pre.max() > 1.0:\n",
    "        img_pre = img_pre / 255.0\n",
    "\n",
    "    # 텐서 변환 (B, C, H, W)\n",
    "    input_tensor = torch.tensor(img_pre, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "    # 4. 모델 추론\n",
    "    model_u.eval(); model_pp.eval()\n",
    "    with torch.no_grad():\n",
    "        out_u = model_u(input_tensor.to(device_u))\n",
    "        if isinstance(out_u, (list, tuple)): out_u = out_u[0]\n",
    "        pred_u_bin = (out_u.squeeze().cpu().numpy() > 0.5).astype(np.uint8)\n",
    "        \n",
    "        out_pp = model_pp(input_tensor.to(device_pp))\n",
    "        if isinstance(out_pp, (list, tuple)): out_pp = out_pp[-1]\n",
    "        pred_pp_bin = (out_pp.squeeze().cpu().numpy() > 0.5).astype(np.uint8)\n",
    "\n",
    "    # 5. IoU 계산 (1채널/3채널 라벨 분기 처리)\n",
    "    iou_u, iou_pp = 0.0, 0.0\n",
    "    if label_path:\n",
    "        label_raw = cv2.imread(label_path, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        # 라벨 채널에 따른 이진 마스크 생성 로직 분기\n",
    "        if label_raw.ndim == 3:\n",
    "            # 라벨이 3채널 RGB인 경우 (특정 색상 매칭)\n",
    "            label_rgb = cv2.cvtColor(label_raw, cv2.COLOR_BGR2RGB)\n",
    "            road_color = [128, 64, 128] # Dataset 로직 참조\n",
    "            target_bin = np.all(label_rgb == road_color, axis=-1).astype(np.uint8)\n",
    "        else:\n",
    "            # 라벨이 1채널인 경우 (값 7 필터링)\n",
    "            target_bin = (label_raw == 7).astype(np.uint8)\n",
    "\n",
    "        # 전처리 적용 (Resize 등)\n",
    "        label_processed = preproc(image=img_rgb, mask=target_bin)\n",
    "        target = label_processed[\"mask\"].astype(np.uint8)\n",
    "        if target.ndim == 3: target = np.squeeze(target, axis=0)\n",
    "            \n",
    "        iou_u = calculate_iou_score(target, pred_u_bin)\n",
    "        iou_pp = calculate_iou_score(target, pred_pp_bin)\n",
    "        print(f\"[{str(idx).zfill(3)}] > {model_type[0]}: {iou_u:.4f}, {model_type[1]}: {iou_pp:.4f}\")\n",
    "\n",
    "    # 6. 시각화 마스크 생성\n",
    "    m_u = cv2.resize(pred_u_bin, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "    m_pp = cv2.resize(pred_pp_bin, (w, h), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    color_mask = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    color_mask[(m_u == 1) & (m_pp == 1)] = [255, 255, 255]\n",
    "    color_mask[(m_u == 1) & (m_pp == 0)] = [255, 0, 0]\n",
    "    color_mask[(m_u == 0) & (m_pp == 1)] = [0, 0, 255]\n",
    "\n",
    "    # 7. 합성 및 저장\n",
    "    background_pil = Image.fromarray(vis_background).convert('RGBA')\n",
    "    mask_pil = Image.fromarray(color_mask).convert('RGBA')\n",
    "    blended_pil = Image.blend(background_pil, mask_pil, alpha=0.5)\n",
    "    \n",
    "    blended_np = np.array(blended_pil.convert('RGB'))\n",
    "    header_h = 50\n",
    "    final_img = np.zeros((h + header_h, w, 3), dtype=np.uint8)\n",
    "    final_img[header_h:, :] = blended_np\n",
    "    \n",
    "    info_text = f\"{model_type[0]}(Red): {iou_u:.2f}, {model_type[1]}(Blue): {iou_pp:.2f}\"\n",
    "    cv2.putText(final_img, info_text, (10, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "    \n",
    "    Image.fromarray(final_img).save(output_path)\n",
    "    return Image.fromarray(final_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee3a119",
   "metadata": {},
   "source": [
    "## UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb785ea9",
   "metadata": {},
   "source": [
    "---\n",
    "#### BCDLoss  \n",
    "기존 방식: 도로영역만 찾는 이진 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "156fb18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 생성\n",
    "# unet_v0 = UNet(input_channels=3, output_channels=1)\n",
    "# unet_v0.to(device)\n",
    "\n",
    "# # 손실 함수 및 옵티마이저 설정\n",
    "# criterion = torch.nn.BCELoss()\n",
    "\n",
    "# train_model(\n",
    "#     unet_v0, train_loader, val_loader, criterion, \n",
    "#     device=device, \n",
    "#     num_epochs=epoch, \n",
    "#     save_dir=save_dir, \n",
    "#     model_type='_unet_bcd'  # 모델 타입 명시\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "503d8d34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (enc1): Unet_block(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (enc2): Unet_block(\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (enc3): Unet_block(\n",
       "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (enc4): Unet_block(\n",
       "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bottleneck): Unet_block(\n",
       "    (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (up6): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (dec6): Unet_block(\n",
       "    (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (up7): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (dec7): Unet_block(\n",
       "    (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (up8): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (dec8): Unet_block(\n",
       "    (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (up9): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (dec9): Unet_block(\n",
       "    (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (final): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = f\"{save_dir}/best_unet_bcd.pth\"\n",
    "unet_v0 = UNet(input_channels=3, output_channels=1)\n",
    "unet_v0.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "unet_v0.eval()  # 평가 모드로 전환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32708812",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = './data/testing'\n",
    "save_train_dir = './data/result/unetV0_train'\n",
    "save_test_dir = './data/result/unetV0_test'\n",
    "\n",
    "os.makedirs(dir_path, exist_ok=True)\n",
    "os.makedirs(save_train_dir, exist_ok=True)\n",
    "os.makedirs(save_test_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47336a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f8300745554df9b5e1b4447b146ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Analysis:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 검증 데이터는 val_dataset.data 리스트를 사용하여 라벨 경로를 함께 넘깁니다.\n",
    "for i in tqdm(range(len(val_dataset.data)), desc=\"Validation Analysis\"):\n",
    "    img_path, label_path = val_dataset.data[i]\n",
    "\n",
    "    get_output(\n",
    "        unet_v0, val_preproc,\n",
    "        image_path=img_path,\n",
    "        output_path= f'{save_train_dir}/result_{str(i).zfill(3)}.png',\n",
    "        label_path=label_path,\n",
    "        im_show = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f800413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b859051c9b6e4f28977bc7a550d14217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Inference Progres:   0%|          | 0/200 [00:00<?, ?img/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 완성한 뒤에는 시각화한 결과를 눈으로 확인해봅시다!\n",
    "for i in tqdm(range(200), desc=\"Test Inference Progres\", unit=\"img\"):\n",
    "    \n",
    "    img_name = f\"00{str(i).zfill(4)}_10.png\"\n",
    "    img_full_path = os.path.join(dir_path, \"image_2\", img_name)\n",
    "    \n",
    "    res_name = f\"result_{str(i).zfill(3)}.png\"\n",
    "    res_full_path = os.path.join(save_test_dir, res_name)\n",
    "\n",
    "    # 추론 실행\n",
    "    get_output(\n",
    "        unet_v0,\n",
    "        val_preproc,\n",
    "        image_path=img_full_path,\n",
    "        output_path=res_full_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a314328",
   "metadata": {},
   "source": [
    "---\n",
    "FocalLoss  \n",
    "개선 방식: 도로영역 뿐만 아니라 도로가 아닌(배경) 영역도 찾는 이진 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc171cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 생성\n",
    "# unet_v1 = UNet(input_channels=3, output_channels=1)\n",
    "# unet_v1.to(device)\n",
    "\n",
    "# # 손실 함수 및 옵티마이저 설정\n",
    "# criterion = FocalLoss(alpha=0.5, gamma=2.0)\n",
    "\n",
    "# train_model(\n",
    "#     unet_v1, train_loader, val_loader, criterion, \n",
    "#     device=device, \n",
    "#     num_epochs=epoch, \n",
    "#     save_dir=save_dir, \n",
    "#     model_type='_unet'  # 모델 타입 명시\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e006c980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (enc1): Unet_block(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (enc2): Unet_block(\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (enc3): Unet_block(\n",
       "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (enc4): Unet_block(\n",
       "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bottleneck): Unet_block(\n",
       "    (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (up6): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (dec6): Unet_block(\n",
       "    (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (up7): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (dec7): Unet_block(\n",
       "    (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (up8): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (dec8): Unet_block(\n",
       "    (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (up9): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (dec9): Unet_block(\n",
       "    (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (final): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = f\"{save_dir}/best_unet.pth\"\n",
    "unet_v1 = UNet(input_channels=3, output_channels=1)\n",
    "unet_v1.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "unet_v1.eval()  # 평가 모드로 전환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1946697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = './data/testing'\n",
    "save_train_dir = './data/result/unetV1_train'\n",
    "save_test_dir = './data/result/unetV1_test'\n",
    "\n",
    "os.makedirs(dir_path, exist_ok=True)\n",
    "os.makedirs(save_train_dir, exist_ok=True)\n",
    "os.makedirs(save_test_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "006609f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13016c37ba1c4582a4fed60bdc63b8b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Analysis:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 검증 데이터는 val_dataset.data 리스트를 사용하여 라벨 경로를 함께 넘깁니다.\n",
    "for i in tqdm(range(len(val_dataset.data)), desc=\"Validation Analysis\"):\n",
    "    img_path, label_path = val_dataset.data[i]\n",
    "\n",
    "    get_output(\n",
    "        unet_v1, val_preproc,\n",
    "        image_path=img_path,\n",
    "        output_path= f'{save_train_dir}/result_{str(i).zfill(3)}.png',\n",
    "        label_path=label_path,\n",
    "        im_show = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1a316e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fddae28a0c142c19dfd083927e0e35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Inference Progres:   0%|          | 0/200 [00:00<?, ?img/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 완성한 뒤에는 시각화한 결과를 눈으로 확인해봅시다!\n",
    "for i in tqdm(range(200), desc=\"Test Inference Progres\", unit=\"img\"):\n",
    "    \n",
    "    img_name = f\"00{str(i).zfill(4)}_10.png\"\n",
    "    img_full_path = os.path.join(dir_path, \"image_2\", img_name)\n",
    "    \n",
    "    res_name = f\"result_{str(i).zfill(3)}.png\"\n",
    "    res_full_path = os.path.join(save_test_dir, res_name)\n",
    "\n",
    "    # 추론 실행\n",
    "    get_output(\n",
    "        unet_v1,\n",
    "        val_preproc,\n",
    "        image_path=img_full_path,\n",
    "        output_path=res_full_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d191d724",
   "metadata": {},
   "source": [
    "##### UNetV2: FocalLoss, KittiDatasetV2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "482fdcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 생성\n",
    "# unet_v2 = UNet(input_channels=3, output_channels=1)\n",
    "# unet_v2.to(device)\n",
    "\n",
    "# # 손실 함수 및 옵티마이저 설정\n",
    "# criterion = FocalLoss(alpha=0.5, gamma=2.0)\n",
    "\n",
    "# train_model(\n",
    "#     unet_v2, train_loader_v2, val_loader_v2, criterion,\n",
    "#     device=device, \n",
    "#     num_epochs=epoch, \n",
    "#     save_dir=save_dir, \n",
    "#     model_type='_unet_v2'  # 모델 타입 명시\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bee54b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (enc1): Unet_block(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (enc2): Unet_block(\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (enc3): Unet_block(\n",
       "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (enc4): Unet_block(\n",
       "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (bottleneck): Unet_block(\n",
       "    (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (up6): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (dec6): Unet_block(\n",
       "    (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (up7): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (dec7): Unet_block(\n",
       "    (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (up8): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (dec8): Unet_block(\n",
       "    (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (up9): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (dec9): Unet_block(\n",
       "    (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (final): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = f\"{save_dir}/best_unet_v2.pth\"\n",
    "unet_v2 = UNet(input_channels=3, output_channels=1)\n",
    "unet_v2.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "unet_v2.eval()  # 평가 모드로 전환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "63c20987",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = './data/testing'\n",
    "save_train_dir = './data/result/unet_v2_train'\n",
    "save_test_dir = './data/result/unet_v2_test'\n",
    "\n",
    "os.makedirs(dir_path, exist_ok=True)\n",
    "os.makedirs(save_train_dir, exist_ok=True)\n",
    "os.makedirs(save_test_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "953c9e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838f3ddffba84b809e36c4b9743e3862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Analysis:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 검증 데이터는 val_dataset.data 리스트를 사용하여 라벨 경로를 함께 넘깁니다.\n",
    "for i in tqdm(range(len(val_dataset_v2.data)), desc=\"Validation Analysis\"):\n",
    "    img_path, label_path = val_dataset_v2.data[i]\n",
    "\n",
    "    get_output(\n",
    "        unet_v2, val_preproc,\n",
    "        image_path=img_path,\n",
    "        output_path= f'{save_train_dir}/result_{str(i).zfill(3)}.png',\n",
    "        label_path=label_path,\n",
    "        im_show = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c72dc202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c8c1a5e3dc4a49aeae7b008cf28667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Inference Progres:   0%|          | 0/200 [00:00<?, ?img/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 완성한 뒤에는 시각화한 결과를 눈으로 확인해봅시다!\n",
    "for i in tqdm(range(200), desc=\"Test Inference Progres\", unit=\"img\"):\n",
    "    \n",
    "    img_name = f\"00{str(i).zfill(4)}_10.png\"\n",
    "    img_full_path = os.path.join(dir_path, \"image_2\", img_name)\n",
    "    \n",
    "    res_name = f\"result_{str(i).zfill(3)}.png\"\n",
    "    res_full_path = os.path.join(save_test_dir, res_name)\n",
    "\n",
    "    # 추론 실행\n",
    "    get_output(\n",
    "        unet_v2,\n",
    "        val_preproc,\n",
    "        image_path=img_full_path,\n",
    "        output_path=res_full_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4541b2d",
   "metadata": {},
   "source": [
    "UNetV0(BCDLOSS) VS UNetV1(FocalLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25905f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c1451d840246888f94717fccc778eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Analysis:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[000] > UNetV0: 0.9465, UNetV1: 0.9107\n",
      "[001] > UNetV0: 0.9159, UNetV1: 0.9062\n",
      "[002] > UNetV0: 0.8009, UNetV1: 0.3720\n",
      "[003] > UNetV0: 0.5532, UNetV1: 0.3091\n",
      "[004] > UNetV0: 0.6218, UNetV1: 0.5683\n",
      "[005] > UNetV0: 0.5525, UNetV1: 0.6175\n",
      "[006] > UNetV0: 0.7380, UNetV1: 0.6998\n",
      "[007] > UNetV0: 0.7074, UNetV1: 0.7174\n",
      "[008] > UNetV0: 0.4448, UNetV1: 0.4511\n",
      "[009] > UNetV0: 0.4638, UNetV1: 0.5695\n",
      "[010] > UNetV0: 0.5357, UNetV1: 0.8215\n",
      "[011] > UNetV0: 0.4966, UNetV1: 0.5662\n",
      "[012] > UNetV0: 0.4695, UNetV1: 0.4817\n",
      "[013] > UNetV0: 0.2082, UNetV1: 0.3208\n",
      "[014] > UNetV0: 0.7823, UNetV1: 0.7491\n",
      "[015] > UNetV0: 0.4928, UNetV1: 0.5732\n",
      "[016] > UNetV0: 0.8846, UNetV1: 0.8910\n",
      "[017] > UNetV0: 0.9441, UNetV1: 0.9103\n",
      "[018] > UNetV0: 0.9204, UNetV1: 0.9078\n",
      "[019] > UNetV0: 0.8529, UNetV1: 0.8110\n",
      "[020] > UNetV0: 0.9403, UNetV1: 0.9319\n",
      "[021] > UNetV0: 0.9805, UNetV1: 0.9824\n",
      "[022] > UNetV0: 0.9474, UNetV1: 0.9501\n",
      "[023] > UNetV0: 0.9450, UNetV1: 0.9558\n",
      "[024] > UNetV0: 0.9267, UNetV1: 0.9582\n",
      "[025] > UNetV0: 0.9425, UNetV1: 0.9551\n",
      "[026] > UNetV0: 0.7526, UNetV1: 0.6989\n",
      "[027] > UNetV0: 0.7439, UNetV1: 0.7390\n",
      "[028] > UNetV0: 0.9165, UNetV1: 0.9048\n",
      "[029] > UNetV0: 0.8058, UNetV1: 0.8169\n",
      "[030] > UNetV0: 0.6862, UNetV1: 0.8879\n",
      "[031] > UNetV0: 0.8218, UNetV1: 0.8754\n",
      "[032] > UNetV0: 0.4223, UNetV1: 0.7871\n",
      "[033] > UNetV0: 0.6029, UNetV1: 0.7674\n",
      "[034] > UNetV0: 0.6589, UNetV1: 0.7522\n",
      "[035] > UNetV0: 0.8761, UNetV1: 0.9371\n",
      "[036] > UNetV0: 0.8108, UNetV1: 0.7927\n",
      "[037] > UNetV0: 0.5583, UNetV1: 0.8000\n",
      "[038] > UNetV0: 0.7830, UNetV1: 0.7686\n",
      "[039] > UNetV0: 0.6749, UNetV1: 0.8518\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터는 val_dataset.data 리스트를 사용하여 라벨 경로를 함께 넘깁니다.\n",
    "compare_val_dir = os.path.join('./', 'data', 'result', 'comparison_results', 'unetv0,1_val')\n",
    "os.makedirs(compare_val_dir, exist_ok=True)\n",
    "\n",
    "for i in tqdm(range(len(val_dataset.data)), desc=\"Validation Analysis\"):\n",
    "    img_path, label_path = val_dataset.data[i]\n",
    "    res_full_path = os.path.join(compare_val_dir, f\"val_compare_{str(i).zfill(2)}.png\")\n",
    "\n",
    "    get_comparison_output(\n",
    "        unet_v0, unet_v1, val_preproc,\n",
    "        image_path=img_path,\n",
    "        output_path=res_full_path,\n",
    "        model_type=['UNetV0','UNetV1'],\n",
    "        label_path=label_path, # 라벨 전달 -> IoU 및 텍스트 출력 활성화\n",
    "        idx=i\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "12d5b6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d0f2fb336447e1a60a2eea6e8f1047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Inference Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test 데이터\n",
    "compare_test_dir = os.path.join('./', 'data', 'result', 'comparison_results', 'unetv0,1_test')\n",
    "os.makedirs(compare_test_dir, exist_ok=True)\n",
    "\n",
    "for i in tqdm(range(200), desc=\"Test Inference Progress\"):\n",
    "    img_name = f\"00{str(i).zfill(4)}_10.png\"\n",
    "    img_full_path = os.path.join(dir_path, \"image_2\", img_name)\n",
    "    res_full_path = os.path.join(compare_test_dir, f\"result_{str(i).zfill(3)}.png\")\n",
    "\n",
    "    get_comparison_output(\n",
    "        unet_v0, unet_v1, val_preproc,\n",
    "        image_path=img_full_path,\n",
    "        output_path=res_full_path,\n",
    "        model_type=['UNetV0','UNetV1'],\n",
    "        label_path=None, # 테스트는 라벨 없음\n",
    "        idx=i\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01afa187",
   "metadata": {},
   "source": [
    "UNetV1(KittiDataset) VS UNeV2(KittiDatasetV2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dbd719d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4065d3f4df1439dbbcbac91d674d620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Analysis:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[000] > UNetV1: 0.9107, UNetV2: 0.9582\n",
      "[001] > UNetV1: 0.9062, UNetV2: 0.9156\n",
      "[002] > UNetV1: 0.3720, UNetV2: 0.8812\n",
      "[003] > UNetV1: 0.3091, UNetV2: 0.4966\n",
      "[004] > UNetV1: 0.5683, UNetV2: 0.8269\n",
      "[005] > UNetV1: 0.6175, UNetV2: 0.8397\n",
      "[006] > UNetV1: 0.6998, UNetV2: 0.7478\n",
      "[007] > UNetV1: 0.7174, UNetV2: 0.9340\n",
      "[008] > UNetV1: 0.4511, UNetV2: 0.5041\n",
      "[009] > UNetV1: 0.5695, UNetV2: 0.4146\n",
      "[010] > UNetV1: 0.8215, UNetV2: 0.8714\n",
      "[011] > UNetV1: 0.5662, UNetV2: 0.7465\n",
      "[012] > UNetV1: 0.4817, UNetV2: 0.7850\n",
      "[013] > UNetV1: 0.3208, UNetV2: 0.5310\n",
      "[014] > UNetV1: 0.7491, UNetV2: 0.9190\n",
      "[015] > UNetV1: 0.5732, UNetV2: 0.8546\n",
      "[016] > UNetV1: 0.8910, UNetV2: 0.8976\n",
      "[017] > UNetV1: 0.9103, UNetV2: 0.9404\n",
      "[018] > UNetV1: 0.9078, UNetV2: 0.9108\n",
      "[019] > UNetV1: 0.8110, UNetV2: 0.8708\n",
      "[020] > UNetV1: 0.9319, UNetV2: 0.9621\n",
      "[021] > UNetV1: 0.9824, UNetV2: 0.9835\n",
      "[022] > UNetV1: 0.9501, UNetV2: 0.9563\n",
      "[023] > UNetV1: 0.9558, UNetV2: 0.9763\n",
      "[024] > UNetV1: 0.9582, UNetV2: 0.9753\n",
      "[025] > UNetV1: 0.9551, UNetV2: 0.9771\n",
      "[026] > UNetV1: 0.6989, UNetV2: 0.7875\n",
      "[027] > UNetV1: 0.7390, UNetV2: 0.7994\n",
      "[028] > UNetV1: 0.9048, UNetV2: 0.9654\n",
      "[029] > UNetV1: 0.8169, UNetV2: 0.9258\n",
      "[030] > UNetV1: 0.8879, UNetV2: 0.9737\n",
      "[031] > UNetV1: 0.8754, UNetV2: 0.9789\n",
      "[032] > UNetV1: 0.7871, UNetV2: 0.9594\n",
      "[033] > UNetV1: 0.7674, UNetV2: 0.8766\n",
      "[034] > UNetV1: 0.7522, UNetV2: 0.9608\n",
      "[035] > UNetV1: 0.9371, UNetV2: 0.9440\n",
      "[036] > UNetV1: 0.7927, UNetV2: 0.8324\n",
      "[037] > UNetV1: 0.8000, UNetV2: 0.7729\n",
      "[038] > UNetV1: 0.7686, UNetV2: 0.8153\n",
      "[039] > UNetV1: 0.8518, UNetV2: 0.9267\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터는 val_dataset.data 리스트를 사용하여 라벨 경로를 함께 넘깁니다.\n",
    "compare_val_dir = os.path.join('./', 'data', 'result', 'comparison_results', 'unetv1,2_val')\n",
    "os.makedirs(compare_val_dir, exist_ok=True)\n",
    "\n",
    "for i in tqdm(range(len(val_dataset_v2.data)), desc=\"Validation Analysis\"):\n",
    "    img_path, label_path = val_dataset_v2.data[i]\n",
    "    res_full_path = os.path.join(compare_val_dir, f\"val_compare_{str(i).zfill(2)}.png\")\n",
    "\n",
    "    get_comparison_output(\n",
    "        unet_v1, unet_v2, val_preproc,\n",
    "        image_path=img_path,\n",
    "        output_path=res_full_path,\n",
    "        model_type=['UNetV1','UNetV2'],\n",
    "        label_path=label_path, # 라벨 전달 -> IoU 및 텍스트 출력 활성화\n",
    "        idx=i\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9474a17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31b57a489504dfda25bfcf3ea7ff1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Inference Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test 데이터\n",
    "compare_test_dir = os.path.join('./', 'data', 'result', 'comparison_results', 'unetv1,2_test')\n",
    "os.makedirs(compare_test_dir, exist_ok=True)\n",
    "\n",
    "for i in tqdm(range(200), desc=\"Test Inference Progress\"):\n",
    "    img_name = f\"00{str(i).zfill(4)}_10.png\"\n",
    "    img_full_path = os.path.join(dir_path, \"image_2\", img_name)\n",
    "    res_full_path = os.path.join(compare_test_dir, f\"result_{str(i).zfill(3)}.png\")\n",
    "\n",
    "    get_comparison_output(\n",
    "        unet_v1, unet_v2, val_preproc,\n",
    "        image_path=img_full_path,\n",
    "        output_path=res_full_path,\n",
    "        model_type=['UNetV1','UNetV2'],\n",
    "        label_path=None, # 테스트는 라벨 없음\n",
    "        idx=i\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd314b",
   "metadata": {},
   "source": [
    "#### UNet ++\n",
    "FocalLoss, KittiDatasetV2 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2393b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 모델 생성\n",
    "# unet_pp = Nested_UNet(input_channels=3, output_channels=1).to(device)\n",
    "\n",
    "# # 손실 함수 및 옵티마이저 설정\n",
    "# criterion = FocalLoss(alpha=0.5, gamma=2.0)\n",
    "\n",
    "# train_model(\n",
    "#     unet_pp, train_loader_v2, val_loader_v2, criterion, \n",
    "#     device=device, \n",
    "#     num_epochs=epoch, \n",
    "#     save_dir=save_dir, \n",
    "#     model_type='_unet_pp'  # 모델 타입 명시\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7fbece18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nested_UNet(\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (up): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "  (conv0_0): Unet_block(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv1_0): Unet_block(\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv2_0): Unet_block(\n",
       "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv3_0): Unet_block(\n",
       "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv4_0): Unet_block(\n",
       "    (conv1): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv0_1): Unet_block(\n",
       "    (conv1): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv1_1): Unet_block(\n",
       "    (conv1): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv2_1): Unet_block(\n",
       "    (conv1): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv3_1): Unet_block(\n",
       "    (conv1): Conv2d(1536, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv0_2): Unet_block(\n",
       "    (conv1): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv1_2): Unet_block(\n",
       "    (conv1): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv2_2): Unet_block(\n",
       "    (conv1): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv0_3): Unet_block(\n",
       "    (conv1): Conv2d(320, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv1_3): Unet_block(\n",
       "    (conv1): Conv2d(640, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv0_4): Unet_block(\n",
       "    (conv1): Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "  )\n",
       "  (output): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = f\"{save_dir}/best_unet_pp.pth\"\n",
    "unet_pp = Nested_UNet(input_channels=3, output_channels=1)\n",
    "unet_pp.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "unet_pp.eval()  # 평가 모드로 전환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "08e4bc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = './data/testing'\n",
    "save_train_dir = './data/result/unet_pp_train'\n",
    "save_test_dir = './data/result/unet_pp_test'\n",
    "\n",
    "os.makedirs(dir_path, exist_ok=True)\n",
    "os.makedirs(save_train_dir, exist_ok=True)\n",
    "os.makedirs(save_test_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ac7bd305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1868e6206ab49a3a3d7d071062749bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Analysis:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 검증 데이터는 val_dataset.data 리스트를 사용하여 라벨 경로를 함께 넘깁니다.\n",
    "for i in tqdm(range(len(val_dataset_v2.data)), desc=\"Validation Analysis\"):\n",
    "    img_path, label_path = val_dataset_v2.data[i]\n",
    "\n",
    "    get_output(\n",
    "        unet_pp, val_preproc,\n",
    "        image_path=img_path,\n",
    "        output_path= f'{save_train_dir}/result_{str(i).zfill(3)}.png',\n",
    "        label_path=label_path,\n",
    "        im_show = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "94efaf15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f092184c55e44f4b86cde997c4ed2421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Inference Progres:   0%|          | 0/200 [00:00<?, ?img/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(range(200), desc=\"Test Inference Progres\", unit=\"img\"):\n",
    "    \n",
    "    img_name = f\"00{str(i).zfill(4)}_10.png\"\n",
    "    img_full_path = os.path.join(dir_path, \"image_2\", img_name)\n",
    "    \n",
    "    res_name = f\"result_{str(i).zfill(3)}.png\"\n",
    "    res_full_path = os.path.join(save_test_dir, res_name)\n",
    "\n",
    "    # 추론 실행\n",
    "    get_output(\n",
    "        unet_pp,\n",
    "        val_preproc,\n",
    "        image_path=img_full_path,\n",
    "        output_path=res_full_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5e4fe711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbfb7030fac0441787840005028cb804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Test Inference Progress:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test 데이터\n",
    "compare_test_dir = os.path.join('./', 'data', 'result', 'comparison_results', 'UNetP_test')\n",
    "os.makedirs(compare_test_dir, exist_ok=True)\n",
    "\n",
    "for i in tqdm(range(200), desc=\"Test Inference Progress\"):\n",
    "    img_name = f\"00{str(i).zfill(4)}_10.png\"\n",
    "    img_full_path = os.path.join(dir_path, \"image_2\", img_name)\n",
    "    res_full_path = os.path.join(compare_test_dir, f\"result_{str(i).zfill(3)}.png\")\n",
    "\n",
    "    get_comparison_output(\n",
    "        unet_v2, unet_pp, val_preproc,\n",
    "        image_path=img_full_path,\n",
    "        output_path=res_full_path,\n",
    "        model_type=['UNetV2','UNet++'],\n",
    "        label_path=None, # 테스트는 라벨 없음\n",
    "        idx=i\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "48a4a8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "893f5c52495d4a5893b0a5d6df037ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Analysis:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[000] > UNetV2: 0.9582, UNet++: 0.9560\n",
      "[001] > UNetV2: 0.9156, UNet++: 0.8686\n",
      "[002] > UNetV2: 0.8812, UNet++: 0.9393\n",
      "[003] > UNetV2: 0.4966, UNet++: 0.4883\n",
      "[004] > UNetV2: 0.8269, UNet++: 0.7912\n",
      "[005] > UNetV2: 0.8397, UNet++: 0.5719\n",
      "[006] > UNetV2: 0.7478, UNet++: 0.5931\n",
      "[007] > UNetV2: 0.9340, UNet++: 0.8657\n",
      "[008] > UNetV2: 0.5041, UNet++: 0.4916\n",
      "[009] > UNetV2: 0.4146, UNet++: 0.4975\n",
      "[010] > UNetV2: 0.8714, UNet++: 0.8775\n",
      "[011] > UNetV2: 0.7465, UNet++: 0.8561\n",
      "[012] > UNetV2: 0.7850, UNet++: 0.6443\n",
      "[013] > UNetV2: 0.5310, UNet++: 0.5453\n",
      "[014] > UNetV2: 0.9190, UNet++: 0.9028\n",
      "[015] > UNetV2: 0.8546, UNet++: 0.7817\n",
      "[016] > UNetV2: 0.8976, UNet++: 0.8965\n",
      "[017] > UNetV2: 0.9404, UNet++: 0.8411\n",
      "[018] > UNetV2: 0.9108, UNet++: 0.8422\n",
      "[019] > UNetV2: 0.8708, UNet++: 0.8707\n",
      "[020] > UNetV2: 0.9621, UNet++: 0.9488\n",
      "[021] > UNetV2: 0.9835, UNet++: 0.9779\n",
      "[022] > UNetV2: 0.9563, UNet++: 0.9607\n",
      "[023] > UNetV2: 0.9763, UNet++: 0.9745\n",
      "[024] > UNetV2: 0.9753, UNet++: 0.9692\n",
      "[025] > UNetV2: 0.9771, UNet++: 0.9767\n",
      "[026] > UNetV2: 0.7875, UNet++: 0.8007\n",
      "[027] > UNetV2: 0.7994, UNet++: 0.8747\n",
      "[028] > UNetV2: 0.9654, UNet++: 0.9682\n",
      "[029] > UNetV2: 0.9258, UNet++: 0.9327\n",
      "[030] > UNetV2: 0.9737, UNet++: 0.9608\n",
      "[031] > UNetV2: 0.9789, UNet++: 0.9716\n",
      "[032] > UNetV2: 0.9594, UNet++: 0.9537\n",
      "[033] > UNetV2: 0.8766, UNet++: 0.8288\n",
      "[034] > UNetV2: 0.9608, UNet++: 0.9086\n",
      "[035] > UNetV2: 0.9440, UNet++: 0.9531\n",
      "[036] > UNetV2: 0.8324, UNet++: 0.7989\n",
      "[037] > UNetV2: 0.7729, UNet++: 0.7204\n",
      "[038] > UNetV2: 0.8153, UNet++: 0.7091\n",
      "[039] > UNetV2: 0.9267, UNet++: 0.8903\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터는 val_dataset.data 리스트를 사용하여 라벨 경로를 함께 넘깁니다.\n",
    "compare_val_dir = os.path.join('./', 'data', 'result', 'comparison_results', 'UNetP_val')\n",
    "os.makedirs(compare_val_dir, exist_ok=True)\n",
    "\n",
    "for i in tqdm(range(len(val_dataset_v2.data)), desc=\"Validation Analysis\"):\n",
    "    img_path, label_path = val_dataset_v2.data[i]\n",
    "    res_full_path = os.path.join(compare_val_dir, f\"val_compare_{str(i).zfill(2)}.png\")\n",
    "\n",
    "    get_comparison_output(\n",
    "        unet_v2, unet_pp, val_preproc,\n",
    "        image_path=img_path,\n",
    "        output_path=res_full_path,\n",
    "        model_type=['UNetV2','UNet++'],\n",
    "        label_path=label_path, # 라벨 전달 -> IoU 및 텍스트 출력 활성화\n",
    "        idx=i\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
