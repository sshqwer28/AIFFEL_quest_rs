{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b97dda3",
   "metadata": {},
   "source": [
    "### 프로젝트 변경점\n",
    "- 사전 학습된 ResNet50을 사용(기존 v1 > v2로 변경)\n",
    "- optimizer 변경(기존 SGD > AdamW로 변경)\n",
    "- 학습률 스케줄러 추가\n",
    "\n",
    "### 프로젝트 결과 분석\n",
    "- 가정: 모델이 충분히 학습이 되었다는 전제하에 CAM과 Grad GAM의 결과를 분석하기\n",
    "- CAM과 Grad CAM의 결과는 동일하게 나타나는 것을 볼 수 있다.\n",
    "    -  이유: 현재 ResNet 구조를 보면, 마지막 Layer에서 AVG(1,1,c)를 사용하고 있기 때문에 GAP을 사용한 것과 같은 결과가 나온다.  \n",
    "    만약 ResNet의 구조를 변경한다면, CAM과 Grad CAM에서 다른 결과를 확인할 수 있을 것이다. 하지만 CAM과 Grad GAM의 결과를 Ablation Study 관점에서 비교하기 어렵다는 문제가 있다.\n",
    "\n",
    "- 여기서 주의깊게 살펴볼 것은 Grad CAM이다.  \n",
    "    - 입력층과 가까운 저차원 레이어는 에지나 텍스처 같은 국소적인 기하학적 특징을 추출하며, 이는 히트맵과 바운딩 박스를 통해 모델이 이미지의 전반적인 기초 윤곽을 탐색하고 있음을 보여준다.  \n",
    "    - 반대로 출력층에 가까운 레이어(고차원)는 레이어를 거치며 극대화된 Receptive Field을 통해 이미지 전체의 문맥을 통합적으로 수용하며, 고차원의 의미론적 추론을 바탕으로 압축된 특징 맵 속에서 객체의 본질적 형상에만 정보를 집중시켜 인식의 범위를 정교하게 수렴시킨다.\n",
    "\n",
    "- 사전 학습된 ResNet 모델의 학습 과정에서 훈련 손실($L_{train}$)과 정확도는 최적점으로 점근적 수렴을 보이나, 검증(Validation) 지표는 특정 시점 이후 최적화 정체기(Optimization Plateau)에 진입한 것을 알 수 있다.  \n",
    "\n",
    "    특히 AdamW의 가중치 감쇠(Weight Decay) 기법이 검증 손실의 급격한 발산을 억제하여 전형적인 과대적합 양상은 나타나지 않지만, 학습이 지속될수록 모델이 객체의 본질적 형상이 아닌 훈련 데이터 특유의 비본질적 노이즈를 암기하게 된다. 이로 인해 수치상의 안정에도 불구하고 실질적인 객체 변별력이 약화되는 일반화 성능 저하가 발생하며, 결과적으로 에포크가 증가할수록 타겟 객체에 대한 탐지 정밀도가 하락하는 결과가 발생한 것을 알 수 있다.\n",
    "\n",
    "\n",
    "### 회고 \n",
    "입력층에 인접한 저차원 레이어에서 에지나 텍스처 같은 국소적 특징을 추출하고, 이를 계층적으로 결합하여 고차원 레이어에서 객체의 공간적 위상과 고차원적 의미론적 추론을 수행하는 과정이 인상적이었다.  \n",
    "특히, 초기 레이어의 시각적 파편들이 여러 단계의 필터를 거치며 Receptive Field를 확장하고, 객체를 인식해나가는 과정에서 인공신경망이 단순한 연산을 넘어 인간의 추론 방식을 공학적으로 재현하고 있음을 알게 되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34bc2684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python, numpy, torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21dcd0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\n",
    "# !wget http://vision.stanford.edu/aditya86/ImageNetDogs/annotation.tar\n",
    "# !wget http://vision.stanford.edu/aditya86/ImageNetDogs/lists.tar\n",
    "# !tar -xf images.tar -C ./\n",
    "# !tar -xf annotation.tar -C ./\n",
    "# !tar -xf lists.tar -C ./\n",
    "\n",
    "# print(\"데이터 다운로드 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33559f5a",
   "metadata": {},
   "source": [
    "## 프로젝트: CAM을 만들고 평가해 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a318138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/torch_gpu/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0+cu128\n",
      "2.4.2\n",
      "4.13.0\n",
      "12.1.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import cv2\n",
    "import PIL\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "\n",
    "print(torch.__version__)\n",
    "print(np.__version__)\n",
    "print(cv2.__version__)\n",
    "print(PIL.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f415d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ccce29",
   "metadata": {},
   "source": [
    "#### 데이터 분류\n",
    "- stanford_dogs 폴더가 없을 때 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06f4b966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 원본 데이터 경로 (압축 해제 후 폴더)\n",
    "# images_dir = './Images'  # 이미지들이 위치한 폴더\n",
    "# mat_dir = './' # 프로젝트 루트 디렉토리\n",
    "\n",
    "# # .mat 파일 경로 (train_list.mat와 test_list.mat가 각각 같은 구조라고 가정)\n",
    "# train_mat_path = os.path.join(mat_dir, 'train_list.mat')\n",
    "# test_mat_path = os.path.join(mat_dir, 'test_list.mat')\n",
    "\n",
    "# # .mat 파일 로드\n",
    "# train_mat = sio.loadmat(train_mat_path)\n",
    "# test_mat = sio.loadmat(test_mat_path)\n",
    "\n",
    "# # train_mat와 test_mat 내부에 'file_list'와 'labels' 등이 있음\n",
    "# train_file_list = train_mat['file_list']\n",
    "# train_labels = train_mat['labels'].squeeze()  # (N,)\n",
    "# test_file_list = test_mat['file_list']\n",
    "# test_labels = test_mat['labels'].squeeze()\n",
    "\n",
    "# # 최종적으로 ImageFolder 구조로 재구성할 대상 폴더 생성 (예: stanford_dogs/train, stanford_dogs/test)\n",
    "# base_dir = 'stanford_dogs'\n",
    "# train_dir = os.path.join(base_dir, 'train')\n",
    "# test_dir = os.path.join(base_dir, 'test')\n",
    "\n",
    "# os.makedirs(train_dir, exist_ok=True)\n",
    "# os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# def process_mat_file(file_list_array, split_dir):\n",
    "#     \"\"\"\n",
    "#     .mat 파일에서 로드한 file_list_array를 사용해,\n",
    "#     split_dir(예: train 혹은 test)에 클래스별 폴더를 생성하고 이미지를 복사합니다.\n",
    "#     \"\"\"\n",
    "#     num_files = file_list_array.shape[0]\n",
    "#     for idx in range(num_files):\n",
    "#         # file_list_array[idx]는 보통 array([<파일경로>]) 형태입니다.\n",
    "#         # 따라서, array([<파일경로>]).item()을 사용하면 실제 문자열을 얻을 수 있습니다.\n",
    "#         file_path = file_list_array[idx][0].item()\n",
    "\n",
    "#         # 혹시 bytes 타입이면 문자열로 디코딩\n",
    "#         if isinstance(file_path, bytes):\n",
    "#             file_path = file_path.decode('utf-8')\n",
    "\n",
    "#         # 파일 경로 예시: 'n02116738-African_hunting_dog/n02116738_2988.jpg'\n",
    "#         # 클래스 이름은 파일 경로의 최상위 폴더명 (예: 'n02116738-African_hunting_dog')\n",
    "#         class_folder = file_path.split('/')[0]\n",
    "\n",
    "#         # 대상 클래스 폴더 생성\n",
    "#         dest_folder = os.path.join(split_dir, class_folder)\n",
    "#         os.makedirs(dest_folder, exist_ok=True)\n",
    "\n",
    "#         # 원본 이미지 경로: Images 폴더 아래에 file_path 위치\n",
    "#         src_path = os.path.join(images_dir, file_path)\n",
    "#         # 대상 이미지 경로: dest_folder 아래에 원본 파일명 그대로 복사\n",
    "#         dest_path = os.path.join(dest_folder, os.path.basename(file_path))\n",
    "\n",
    "#         # 파일 존재 여부 확인 후 복사\n",
    "#         if os.path.exists(src_path):\n",
    "#             shutil.copy(src_path, dest_path)\n",
    "#         else:\n",
    "#             print(f\"File not found: {src_path}\")\n",
    "\n",
    "# print(\"Processing train set...\")\n",
    "# process_mat_file(train_file_list, train_dir)\n",
    "# print(\"Processing test set...\")\n",
    "# process_mat_file(test_file_list, test_dir)\n",
    "\n",
    "# print(\"Dataset reorganization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "205409e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StanfordDogsDatasetWithBBox(datasets.ImageFolder):\n",
    "    def __init__(self, root, annotation_root, transform=None):\n",
    "        super().__init__(root, transform=transform)\n",
    "        self.annotation_root = annotation_root  # 예: '/content/Annotation'\n",
    "        self.new_size = (224, 224)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image, label = super().__getitem__(index)\n",
    "        path, _ = self.samples[index]\n",
    "\n",
    "        rel_path = os.path.relpath(path, self.root)\n",
    "        annot_filename = os.path.splitext(os.path.basename(rel_path))[0]\n",
    "        annot_folder = os.path.dirname(rel_path)\n",
    "        annot_path = os.path.join(self.annotation_root, annot_folder, annot_filename)\n",
    "\n",
    "        # bbox 기본값 (예: [ymin, xmin, ymax, xmax])\n",
    "        bbox = [0.0, 0.0, 0.0, 0.0]\n",
    "\n",
    "        if os.path.exists(annot_path):\n",
    "            try:\n",
    "                # 확장자가 없지만 XML 형식의 파일이라고 가정하고 파싱\n",
    "                tree = ET.parse(annot_path)\n",
    "                root_xml = tree.getroot()\n",
    "                # 첫 번째 object 태그에서 bndbox 정보를 읽음\n",
    "                obj = root_xml.find('object')\n",
    "                if obj is not None:\n",
    "                    bndbox = obj.find('bndbox')\n",
    "                    if bndbox is not None:\n",
    "                        xmin = float(bndbox.find('xmin').text)\n",
    "                        ymin = float(bndbox.find('ymin').text)\n",
    "                        xmax = float(bndbox.find('xmax').text)\n",
    "                        ymax = float(bndbox.find('ymax').text)\n",
    "                        # XML 내 <size> 태그에서 원본 이미지 크기 획득\n",
    "                        size = root_xml.find('size')\n",
    "                        w = float(size.find('width').text)\n",
    "                        h = float(size.find('height').text)\n",
    "                        new_h, new_w = self.new_size\n",
    "                        # bbox 좌표 순서: [ymin, xmin, ymax, xmax]\n",
    "                        bbox = [xmin * (new_w / w), ymin * (new_h / h),\n",
    "                                xmax * (new_w / w), ymax * (new_h / h)]\n",
    "                    else:\n",
    "                        print(f\"bndbox 태그를 찾을 수 없습니다: {annot_path}\")\n",
    "                else:\n",
    "                    print(f\"object 태그를 찾을 수 없습니다: {annot_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing {annot_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"Annotation file not found: {annot_path}\")\n",
    "\n",
    "        return image, label, bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43899b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize(img_tensor):\n",
    "    \"\"\"\n",
    "    img_tensor: [C, H, W] 텐서 (전처리 상태, 예: normalization 적용됨)\n",
    "    ImageNet 평균 및 표준편차를 이용하여 복원 (RGB 순서)\n",
    "    \"\"\"\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std  = np.array([0.229, 0.224, 0.225])\n",
    "    img = img_tensor.cpu().numpy().transpose(1, 2, 0)\n",
    "    img = std * img + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    img = np.uint8(255 * img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2371ec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox(cam, threshold=0.01):\n",
    "    coords = np.argwhere(cam > threshold)\n",
    "    if coords.size == 0:\n",
    "        return None\n",
    "    # np.argwhere의 결과는 (row, col) 즉, (y, x) 순서입니다.\n",
    "    y_min, x_min = coords.min(axis=0)\n",
    "    y_max, x_max = coords.max(axis=0)\n",
    "    return (x_min, y_min, x_max, y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "196438f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bbox_on_image(img, bbox, box_color=(255, 0, 0), thickness=2):\n",
    "    # 원본 이미지 복사\n",
    "    img_with_bbox = img.copy()\n",
    "    if bbox is not None:\n",
    "        x_min, y_min, x_max, y_max = bbox\n",
    "        cv2.rectangle(img_with_bbox, (x_min, y_min), (x_max, y_max), box_color, thickness)\n",
    "    else:\n",
    "        print(\"활성화된 영역이 없습니다.\")\n",
    "    return img_with_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b4c633d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_both_bbox_on_image(img, bbox, ground_truth, thickness=2):\n",
    "    # 원본 이미지 복사\n",
    "    img_with_bbox = img.copy()\n",
    "\n",
    "    if bbox is not None:\n",
    "        x_min, y_min, x_max, y_max = bbox\n",
    "    # 그리기 로직 진행\n",
    "    else:\n",
    "        print(\"경고: 객체가 탐지되지 않아 바운딩 박스를 생성할 수 없습니다.\")\n",
    "        # 기본값 설정 또는 건너뛰기\n",
    "        x_min, y_min, x_max, y_max = 0, 0, 0, 0\n",
    "    cv2.rectangle(img_with_bbox, (x_min, y_min), (x_max, y_max), (255, 0, 0), thickness)\n",
    "\n",
    "    x_min_t, y_min_t, x_max_t, y_max_t = ground_truth\n",
    "    cv2.rectangle(img_with_bbox, (x_min_t, y_min_t), (x_max_t, y_max_t), (0, 255, 0), thickness)\n",
    "\n",
    "    return img_with_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eb14350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(gt_bbox, pred_bbox):\n",
    "    iou = None\n",
    "    # TODO: get iou between two bbox\n",
    "    # bbox 중 하나라도 None이면 IoU 0.0\n",
    "    if gt_bbox is None or  pred_bbox is None:\n",
    "        return 0.0\n",
    "\n",
    "    x_min1, y_min1, x_max1, y_max1 = gt_bbox\n",
    "    x_min2, y_min2, x_max2, y_max2 =  pred_bbox\n",
    "\n",
    "    # 두 bbox의 교집합 영역 좌표 계산\n",
    "    x_min_inter = max(x_min1, x_min2)\n",
    "    y_min_inter = max(y_min1, y_min2)\n",
    "    x_max_inter = min(x_max1, x_max2)\n",
    "    y_max_inter = min(y_max1, y_max2)\n",
    "\n",
    "    # 교집합의 너비와 높이 (음수가 되지 않도록)\n",
    "    inter_width = max(0, x_max_inter - x_min_inter)\n",
    "    inter_height = max(0, y_max_inter - y_min_inter)\n",
    "    inter_area = inter_width * inter_height\n",
    "\n",
    "    # 각 bbox의 면적 계산\n",
    "    area1 = (x_max1 - x_min1) * (y_max1 - y_min1)\n",
    "    area2 = (x_max2 - x_min2) * (y_max2 - y_min2)\n",
    "\n",
    "    # 합집합 면적: A ∪ B = A + B - A ∩ B\n",
    "    union_area = area1 + area2 - inter_area\n",
    "    if union_area <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    iou = inter_area / union_area\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07197332",
   "metadata": {},
   "source": [
    "### CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d41f03b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cam(model, item):\n",
    "    cam_image = None\n",
    "    # TODO: generate cam image\n",
    "    model.eval()\n",
    "    features = []\n",
    "\n",
    "    def hook(module, input, output):\n",
    "        features.append(output.detach())\n",
    "    hook_handle = model.layer4.register_forward_hook(hook)\n",
    "\n",
    "    output = model(item)\n",
    "    hook_handle.remove()\n",
    "\n",
    "    pred_class = output.argmax(dim=1).item()\n",
    "\n",
    "    fc_weights = model.fc.weight.data.to(item.device)\n",
    "\n",
    "    fmap = features[0][0]\n",
    "    cam_image = torch.zeros(fmap.shape[1:], dtype=torch.float32, device=item.device)\n",
    "\n",
    "    for i, w in enumerate(fc_weights[pred_class]):\n",
    "        cam_image += w * fmap[i, :, :]\n",
    "    cam_image = cam_image.cpu().numpy()\n",
    "\n",
    "    cam_image = np.maximum(cam_image, 0)\n",
    "    cam_image = (cam_image - np.min(cam_image)) / (np.max(cam_image) - np.min(cam_image) + 1e-8)\n",
    "\n",
    "    return cam_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23a4c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cam(models, image, ground_bbox, item, start_epochs=1, step_epochs=2):\n",
    "    num_epochs = len(models)\n",
    "    rows = 3\n",
    "    cols = num_epochs + 1 \n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))\n",
    "    \n",
    "    orig_img = unnormalize(image)\n",
    "    ground_truth = [int(x) for x in ground_bbox]\n",
    "\n",
    "    axes[0, 0].imshow(orig_img)\n",
    "    axes[0, 0].set_title(\"Original Image\")\n",
    "    axes[0, 0].axis(\"off\")\n",
    "    \n",
    "    axes[1, 0].imshow(orig_img)\n",
    "    axes[1, 0].set_title(\"Original (For Overlay Ref)\")\n",
    "    axes[1, 0].axis(\"off\")\n",
    "    \n",
    "    gt_only_img = visualize_bbox_on_image(orig_img.copy(), ground_truth) if 'visualize_bbox_on_image' in globals() else orig_img\n",
    "    axes[2, 0].imshow(gt_only_img)\n",
    "    axes[2, 0].set_title(\"Ground Truth BBox\")\n",
    "    axes[2, 0].axis(\"off\")\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        col_idx = i + 1\n",
    "        epoch_num = start_epochs + (i * step_epochs)\n",
    "        \n",
    "        cam = generate_cam(model, item)\n",
    "        cam_resized = cv2.resize(cam, (orig_img.shape[1], orig_img.shape[0]))\n",
    "        cam_bbox = get_bbox(cam_resized, threshold=0.5)\n",
    "        \n",
    "        axes[0, col_idx].imshow(cam)\n",
    "        axes[0, col_idx].set_title(f\"Epoch {epoch_num} CAM\")\n",
    "        axes[0, col_idx].axis(\"off\")\n",
    "        \n",
    "        axes[1, col_idx].imshow(orig_img)\n",
    "        axes[1, col_idx].imshow(cam_resized, cmap='jet', alpha=0.5) \n",
    "        axes[1, col_idx].set_title(f\"Epoch {epoch_num} Overlay\")\n",
    "        axes[1, col_idx].axis(\"off\")\n",
    "        \n",
    "        iou_score = get_iou(cam_bbox, ground_truth) \n",
    "        cam_img_bbox = visualize_both_bbox_on_image(orig_img.copy(), cam_bbox, ground_truth)\n",
    "        \n",
    "        axes[2, col_idx].imshow(cam_img_bbox)\n",
    "        title_text = f\"Epoch {epoch_num} IOU: {iou_score:.4f}\" if iou_score is not None else f\"Epoch {epoch_num} Compare\"\n",
    "        axes[2, col_idx].set_title(title_text)\n",
    "        axes[2, col_idx].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce3d7f5",
   "metadata": {},
   "source": [
    "### grad cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31d65fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_grad_cam(model,item, activation_layer= \"layer4\"):\n",
    "    # TODO: generate grad_cam_image\n",
    "    model.eval()\n",
    "    features = {}\n",
    "    gradients = {}\n",
    "\n",
    "    # forward hook: 대상 레이어의 출력을 저장\n",
    "    def forward_hook(module, input, output):\n",
    "        features['value'] = output.detach()\n",
    "\n",
    "    # backward hook: 대상 레이어의 gradient를 저장\n",
    "    def backward_hook(module, grad_in, grad_out):\n",
    "        gradients['value'] = grad_out[0].detach()\n",
    "\n",
    "    # 모델 내에서 이름이  activation_layer과 일치하는 레이어 검색\n",
    "    target_layer = dict(model.named_modules()).get( activation_layer, None)\n",
    "    if target_layer is None:\n",
    "        raise ValueError(f\"Layer '{activation_layer}' not found in the model.\")\n",
    "    \n",
    "    # hook 등록\n",
    "    forward_handle = target_layer.register_forward_hook(forward_hook)\n",
    "    backward_handle = target_layer.register_backward_hook(backward_hook)\n",
    "\n",
    "    # 순전파 실행\n",
    "    output = model(item)\n",
    "    pred_class = output.argmax(dim=1).item()\n",
    "\n",
    "    # 예측 클래스에 대해 backward 수행\n",
    "    model.zero_grad()\n",
    "    score = output[0, pred_class]\n",
    "    score.backward()\n",
    "\n",
    "    # hook 제거\n",
    "    forward_handle.remove()\n",
    "    backward_handle.remove()\n",
    "\n",
    "    # 저장된 feature map과 gradient 추출 (shape: [C, H, W])\n",
    "    fmap = features['value'][0]\n",
    "    grads = gradients['value'][0]\n",
    "\n",
    "    # 각 채널에 대해 gradient의 global average pooling 계산 (weight 역할)\n",
    "    weights = torch.mean(grads, dim=(1, 2))\n",
    "\n",
    "    # weighted sum: 각 채널의 feature map에 weight를 곱해 합산\n",
    "    grad_cam_image = torch.zeros(fmap.shape[1:], dtype=torch.float32, device=fmap.device)\n",
    "    for i, w in enumerate(weights):\n",
    "        grad_cam_image += w * fmap[i, :, :]\n",
    "    grad_cam_image = grad_cam_image.cpu().numpy()\n",
    "\n",
    "    # ReLU 적용 및 정규화: 음수 값 제거 및 [0,1] 범위로 스케일링\n",
    "    grad_cam_image = np.maximum(grad_cam_image, 0)\n",
    "    grad_cam_image = (grad_cam_image - grad_cam_image.min()) / (grad_cam_image.max() - grad_cam_image.min() + 1e-8)\n",
    "\n",
    "    return grad_cam_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99341272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad_cam(models_list, image, ground_bbox, item, start_epochs, step_epochs):\n",
    "    layers = [\"layer1\", \"layer2\", \"layer3\", \"layer4\"]\n",
    "    num_epochs = len(models_list)\n",
    "    num_layers = len(layers)\n",
    "    \n",
    "    rows = num_epochs * 3 \n",
    "    cols = num_layers + 1 \n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))\n",
    "    \n",
    "    orig_img = unnormalize(image)\n",
    "    ground_truth = [int(x) for x in ground_bbox]\n",
    "    \n",
    "    gt_only_img = visualize_bbox_on_image(orig_img.copy(), ground_truth) if 'visualize_bbox_on_image' in globals() else orig_img\n",
    "\n",
    "    for e_idx, model in enumerate(models_list):\n",
    "        epoch_num = start_epochs + (e_idx * step_epochs)\n",
    "        base_row = e_idx * 3\n",
    "        \n",
    "        axes[base_row, 0].imshow(orig_img)\n",
    "        axes[base_row, 0].set_title(f\"Ep {epoch_num}\\nOrig Image\")\n",
    "        axes[base_row, 0].axis(\"off\")\n",
    "        \n",
    "        axes[base_row + 1, 0].imshow(orig_img)\n",
    "        axes[base_row + 1, 0].set_title(f\"Ep {epoch_num}\\nOverlay Ref\")\n",
    "        axes[base_row + 1, 0].axis(\"off\")\n",
    "        \n",
    "        axes[base_row + 2, 0].imshow(gt_only_img)\n",
    "        axes[base_row + 2, 0].set_title(f\"Ep {epoch_num}\\nGround Truth\")\n",
    "        axes[base_row + 2, 0].axis(\"off\")\n",
    "\n",
    "        for l_idx, layer_name in enumerate(layers):\n",
    "            col_idx = l_idx + 1\n",
    "            \n",
    "            grad_cam = generate_grad_cam(model, item, activation_layer=layer_name)\n",
    "            grad_cam_resized = cv2.resize(grad_cam, (orig_img.shape[1], orig_img.shape[0]))\n",
    "            grad_cam_bbox = get_bbox(grad_cam_resized, threshold=0.5)\n",
    "            \n",
    "            axes[base_row, col_idx].imshow(grad_cam)\n",
    "            axes[base_row, col_idx].set_title(f\"Ep {epoch_num} {layer_name} CAM\")\n",
    "            axes[base_row, col_idx].axis(\"off\")\n",
    "            \n",
    "            axes[base_row + 1, col_idx].imshow(orig_img)\n",
    "            axes[base_row + 1, col_idx].imshow(grad_cam_resized, cmap='jet', alpha=0.5)\n",
    "            axes[base_row + 1, col_idx].set_title(f\"Ep {epoch_num} {layer_name} Overlay\")\n",
    "            axes[base_row + 1, col_idx].axis(\"off\")\n",
    "            \n",
    "            iou_score = get_iou(grad_cam_bbox, ground_truth)\n",
    "            grad_cam_img_bbox = visualize_both_bbox_on_image(orig_img.copy(), grad_cam_bbox, ground_truth)\n",
    "            \n",
    "            axes[base_row + 2, col_idx].imshow(grad_cam_img_bbox)\n",
    "            title_text = f\"{layer_name} IOU: {iou_score:.4f}\" if iou_score is not None else f\"{layer_name} Compare\"\n",
    "            axes[base_row + 2, col_idx].set_title(title_text)\n",
    "            axes[base_row + 2, col_idx].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d50bcad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 120\n",
      "Train samples: 12000\n",
      "Test samples: 8580\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# 데이터셋 경로 (재구성한 ImageFolder 형식)\n",
    "train_dir = os.path.join('stanford_dogs', 'train')\n",
    "test_dir = os.path.join('stanford_dogs', 'test')\n",
    "\n",
    "# Annotation 폴더 경로 (예: '/root/Annotation')\n",
    "annotation_dir = './Annotation'\n",
    "# 커스텀 데이터셋 생성: image, label, bbox 반환\n",
    "train_dataset = StanfordDogsDatasetWithBBox(root=train_dir, annotation_root=annotation_dir, transform=transform)\n",
    "valid_dataset = StanfordDogsDatasetWithBBox(root=test_dir, annotation_root=annotation_dir, transform=transform)\n",
    "\n",
    "batch_size = 12\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, drop_last= True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=2, drop_last= True)\n",
    "\n",
    "num_classes = len(train_dataset.classes)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "print(f\"Train samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(valid_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "069a80de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "annotation: \n",
      "\t\n",
      "folder: 02085620\n",
      "filename: n02085620_10074\n",
      "source: \n",
      "\t\t\n",
      "database: ImageNet database\n",
      "size: \n",
      "\t\t\n",
      "width: 333\n",
      "height: 500\n",
      "depth: 3\n",
      "segment: 0\n",
      "object: \n",
      "\t\t\n",
      "name: Chihuahua\n",
      "pose: Unspecified\n",
      "truncated: 0\n",
      "difficult: 0\n",
      "bndbox: \n",
      "\t\t\t\n",
      "xmin: 25\n",
      "ymin: 10\n",
      "xmax: 276\n",
      "ymax: 498\n"
     ]
    }
   ],
   "source": [
    "# Annotation 폴더에서 데이터를 골라보세요\n",
    "annotation_path = './Annotation/n02085620-Chihuahua/n02085620_10074'\n",
    "\n",
    "# XML 파일 파싱\n",
    "tree = ET.parse(annotation_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "for elem in root.iter():\n",
    "    print(f\"{elem.tag}: {elem.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b5d0f9",
   "metadata": {},
   "source": [
    "### 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbb3dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=2, model_type= ''):\n",
    "    # 결과 이미지를 저장할 폴더 생성\n",
    "    save_dir = './results'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    total_steps = num_epochs * len(train_loader)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=1e-6)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} 시작\")\n",
    "\n",
    "        for batch_idx, (images, labels, _) in enumerate(train_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(f\"  Step {batch_idx+1} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "            scheduler.step()\n",
    "\n",
    "        train_loss = running_loss / (batch_idx + 1)\n",
    "        train_acc = correct / total\n",
    "        print(f\"Train - Loss: {train_loss:.4f}, Accuracy: {train_acc*100:.2f}%\")\n",
    "\n",
    "        # 검증 단계\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        all_probs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels, _ in valid_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                \n",
    "                # ROC AUC를 위해 Softmax로 확률값 계산\n",
    "                probs = F.softmax(outputs, dim=1)\n",
    "                \n",
    "                # GPU 텐서를 CPU로 옮기고 NumPy 배열로 변환하여 리스트에 저장\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "        # 전체 검증 데이터에 대한 지표 계산\n",
    "        test_loss = running_loss / len(valid_loader)\n",
    "        test_acc = correct / total\n",
    "        \n",
    "        # scikit-learn을 이용한 지표 산출 (다중 분류 기준으로 average='macro' 사용)\n",
    "        y_true = np.array(all_labels)\n",
    "        y_pred = np.array(all_preds)\n",
    "        y_prob = np.array(all_probs)\n",
    "        \n",
    "        val_precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        val_recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        val_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        val_conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        # ROC AUC (다중 분류 처리. 이진 분류일 경우 multi_class 옵션을 빼고 y_prob[:, 1]을 넣어야 할 수 있습니다)\n",
    "        try:\n",
    "            val_roc_auc = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
    "        except ValueError:\n",
    "            val_roc_auc = float('nan')\n",
    "            \n",
    "        print(f\"Validation - Loss: {test_loss:.4f}, Accuracy: {test_acc*100:.2f}%\")\n",
    "        print(f\"             Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1-score: {val_f1:.4f}\")\n",
    "        print(f\"             ROC AUC: {val_roc_auc:.4f}\")\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(24, 20)) \n",
    "\n",
    "        sns.heatmap(val_conf_matrix, annot=False, fmt='d', cmap='Blues',\n",
    "                    xticklabels=5, yticklabels=5) \n",
    "\n",
    "        plt.title(f'Confusion Matrix (Large) - Epoch {epoch+1}', fontsize=20)\n",
    "        plt.xlabel('Predicted Label', fontsize=16)\n",
    "        plt.ylabel('True Label', fontsize=16)\n",
    "        plt.xticks(rotation=90, fontsize=12)\n",
    "        plt.yticks(rotation=0, fontsize=12)\n",
    "\n",
    "        cm_save_path = os.path.join(save_dir, f'confusion_matrix{model_type}_e{epoch+1}.png')\n",
    "        plt.savefig(cm_save_path, bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "        conf_matrix_errors = val_conf_matrix.copy()\n",
    "        np.fill_diagonal(conf_matrix_errors, 0)\n",
    "\n",
    "        plt.figure(figsize=(20, 16)) \n",
    "        sns.heatmap(conf_matrix_errors, annot=False, cmap='Reds', xticklabels=5, yticklabels=5)\n",
    "        \n",
    "        plt.title(f'Confusion Matrix (Errors Only) - Epoch {epoch+1}', fontsize=20)\n",
    "        plt.xlabel('Predicted Label', fontsize=16)\n",
    "        plt.ylabel('True Label', fontsize=16)\n",
    "        \n",
    "        cm_save_path = os.path.join(save_dir, f'confusion_matrix_errors{model_type}_e{epoch+1}.png')\n",
    "        plt.savefig(cm_save_path, bbox_inches='tight') # 잘리는 부분 없이 저장\n",
    "        plt.close()\n",
    "        # 모델 저장\n",
    "        cam_model_path = f'./models/cam_model{model_type}_e{epoch+1}.pt'\n",
    "        folder_path = os.path.dirname(cam_model_path)\n",
    "\n",
    "        if folder_path and not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "\n",
    "        torch.save(model, cam_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44031d46",
   "metadata": {},
   "source": [
    "### ResNet50  - pre trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d745538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.resnet50(pretrained=True)\n",
    "model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(train_dataset.classes))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dc936e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=120, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b9caeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b577ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10 시작\n",
      "  Step 100 - Loss: 4.0608\n",
      "  Step 200 - Loss: 2.8946\n",
      "  Step 300 - Loss: 2.0338\n",
      "  Step 400 - Loss: 2.0954\n",
      "  Step 500 - Loss: 2.1589\n",
      "  Step 600 - Loss: 1.7397\n",
      "  Step 700 - Loss: 1.6869\n",
      "  Step 800 - Loss: 1.5228\n",
      "  Step 900 - Loss: 1.2155\n",
      "  Step 1000 - Loss: 1.6163\n",
      "Train - Loss: 2.2981, Accuracy: 58.65%\n",
      "Validation - Loss: 1.4325, Accuracy: 79.21%\n",
      "             Precision: 0.7945, Recall: 0.7867, F1-score: 0.7760\n",
      "             ROC AUC: 0.9970\n",
      "\n",
      "Epoch 2/10 시작\n",
      "  Step 100 - Loss: 1.0193\n",
      "  Step 200 - Loss: 1.3875\n",
      "  Step 300 - Loss: 1.3552\n",
      "  Step 400 - Loss: 1.0478\n",
      "  Step 500 - Loss: 1.0604\n",
      "  Step 600 - Loss: 1.0308\n",
      "  Step 700 - Loss: 1.3586\n",
      "  Step 800 - Loss: 1.1080\n",
      "  Step 900 - Loss: 1.4823\n",
      "  Step 1000 - Loss: 0.9837\n",
      "Train - Loss: 1.2988, Accuracy: 83.92%\n",
      "Validation - Loss: 1.3806, Accuracy: 80.89%\n",
      "             Precision: 0.8091, Recall: 0.8018, F1-score: 0.7972\n",
      "             ROC AUC: 0.9974\n",
      "\n",
      "Epoch 3/10 시작\n",
      "  Step 100 - Loss: 1.0955\n",
      "  Step 200 - Loss: 1.1142\n",
      "  Step 300 - Loss: 0.9961\n",
      "  Step 400 - Loss: 1.0595\n",
      "  Step 500 - Loss: 0.9429\n",
      "  Step 600 - Loss: 1.1883\n",
      "  Step 700 - Loss: 1.2123\n",
      "  Step 800 - Loss: 1.3464\n",
      "  Step 900 - Loss: 1.5162\n",
      "  Step 1000 - Loss: 1.2484\n",
      "Train - Loss: 1.0769, Accuracy: 91.83%\n",
      "Validation - Loss: 1.3730, Accuracy: 82.10%\n",
      "             Precision: 0.8198, Recall: 0.8153, F1-score: 0.8114\n",
      "             ROC AUC: 0.9961\n",
      "\n",
      "Epoch 4/10 시작\n",
      "  Step 100 - Loss: 0.8770\n",
      "  Step 200 - Loss: 1.1034\n",
      "  Step 300 - Loss: 0.9427\n",
      "  Step 400 - Loss: 0.8551\n",
      "  Step 500 - Loss: 1.0671\n",
      "  Step 600 - Loss: 1.0708\n",
      "  Step 700 - Loss: 0.8692\n",
      "  Step 800 - Loss: 0.8382\n",
      "  Step 900 - Loss: 0.9620\n",
      "  Step 1000 - Loss: 1.0567\n",
      "Train - Loss: 0.9647, Accuracy: 95.74%\n",
      "Validation - Loss: 1.4165, Accuracy: 80.49%\n",
      "             Precision: 0.8052, Recall: 0.8054, F1-score: 0.7982\n",
      "             ROC AUC: 0.9955\n",
      "\n",
      "Epoch 5/10 시작\n",
      "  Step 100 - Loss: 0.8750\n",
      "  Step 200 - Loss: 0.9163\n",
      "  Step 300 - Loss: 0.8494\n",
      "  Step 400 - Loss: 0.9571\n",
      "  Step 500 - Loss: 0.8535\n",
      "  Step 600 - Loss: 0.8614\n",
      "  Step 700 - Loss: 0.8889\n",
      "  Step 800 - Loss: 0.8467\n",
      "  Step 900 - Loss: 0.8713\n",
      "  Step 1000 - Loss: 0.8629\n",
      "Train - Loss: 0.8980, Accuracy: 98.02%\n",
      "Validation - Loss: 1.3918, Accuracy: 81.38%\n",
      "             Precision: 0.8101, Recall: 0.8094, F1-score: 0.8050\n",
      "             ROC AUC: 0.9950\n",
      "\n",
      "Epoch 6/10 시작\n",
      "  Step 100 - Loss: 0.8430\n",
      "  Step 200 - Loss: 1.1791\n",
      "  Step 300 - Loss: 0.8698\n",
      "  Step 400 - Loss: 0.9146\n",
      "  Step 500 - Loss: 0.8223\n",
      "  Step 600 - Loss: 0.8221\n",
      "  Step 700 - Loss: 0.8263\n",
      "  Step 800 - Loss: 0.9263\n",
      "  Step 900 - Loss: 0.8273\n",
      "  Step 1000 - Loss: 0.8378\n",
      "Train - Loss: 0.8599, Accuracy: 99.23%\n",
      "Validation - Loss: 1.3825, Accuracy: 82.12%\n",
      "             Precision: 0.8163, Recall: 0.8170, F1-score: 0.8131\n",
      "             ROC AUC: 0.9949\n",
      "\n",
      "Epoch 7/10 시작\n",
      "  Step 100 - Loss: 0.8365\n",
      "  Step 200 - Loss: 0.8403\n",
      "  Step 300 - Loss: 0.8242\n",
      "  Step 400 - Loss: 0.8267\n",
      "  Step 500 - Loss: 0.8890\n",
      "  Step 600 - Loss: 0.8566\n",
      "  Step 700 - Loss: 0.8168\n",
      "  Step 800 - Loss: 0.8276\n",
      "  Step 900 - Loss: 0.8209\n",
      "  Step 1000 - Loss: 0.8501\n",
      "Train - Loss: 0.8393, Accuracy: 99.53%\n",
      "Validation - Loss: 1.3787, Accuracy: 82.88%\n",
      "             Precision: 0.8234, Recall: 0.8235, F1-score: 0.8212\n",
      "             ROC AUC: 0.9944\n",
      "\n",
      "Epoch 8/10 시작\n",
      "  Step 100 - Loss: 0.8190\n",
      "  Step 200 - Loss: 0.8279\n",
      "  Step 300 - Loss: 0.8141\n",
      "  Step 400 - Loss: 0.8289\n",
      "  Step 500 - Loss: 0.8175\n",
      "  Step 600 - Loss: 0.8138\n",
      "  Step 700 - Loss: 0.8342\n",
      "  Step 800 - Loss: 0.8084\n",
      "  Step 900 - Loss: 0.9815\n",
      "  Step 1000 - Loss: 0.8192\n",
      "Train - Loss: 0.8308, Accuracy: 99.71%\n",
      "Validation - Loss: 1.3805, Accuracy: 82.93%\n",
      "             Precision: 0.8258, Recall: 0.8258, F1-score: 0.8227\n",
      "             ROC AUC: 0.9944\n",
      "\n",
      "Epoch 9/10 시작\n",
      "  Step 100 - Loss: 0.9016\n",
      "  Step 200 - Loss: 0.8157\n",
      "  Step 300 - Loss: 0.8338\n",
      "  Step 400 - Loss: 0.8135\n",
      "  Step 500 - Loss: 0.8151\n",
      "  Step 600 - Loss: 0.8149\n",
      "  Step 700 - Loss: 0.8188\n",
      "  Step 800 - Loss: 0.8115\n",
      "  Step 900 - Loss: 0.9158\n",
      "  Step 1000 - Loss: 0.8206\n",
      "Train - Loss: 0.8240, Accuracy: 99.79%\n",
      "Validation - Loss: 1.3791, Accuracy: 82.98%\n",
      "             Precision: 0.8257, Recall: 0.8250, F1-score: 0.8230\n",
      "             ROC AUC: 0.9941\n",
      "\n",
      "Epoch 10/10 시작\n",
      "  Step 100 - Loss: 0.8157\n",
      "  Step 200 - Loss: 0.8182\n",
      "  Step 300 - Loss: 0.8786\n",
      "  Step 400 - Loss: 0.8223\n",
      "  Step 500 - Loss: 0.8085\n",
      "  Step 600 - Loss: 0.8190\n",
      "  Step 700 - Loss: 0.8245\n",
      "  Step 800 - Loss: 0.8295\n",
      "  Step 900 - Loss: 0.8260\n",
      "  Step 1000 - Loss: 0.8161\n",
      "Train - Loss: 0.8220, Accuracy: 99.83%\n",
      "Validation - Loss: 1.3712, Accuracy: 83.08%\n",
      "             Precision: 0.8271, Recall: 0.8271, F1-score: 0.8248\n",
      "             ROC AUC: 0.9940\n",
      "CPU times: user 18min 26s, sys: 1min 44s, total: 20min 11s\n",
      "Wall time: 20min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=max_epochs, model_type = \"_pre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfba572f",
   "metadata": {},
   "source": [
    "### CAM, Grad CAM 체크"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a4f8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation dataset 확인용\n",
    "def show_dataset_grid(dataset, start_index=0, num_images=12, cols=4):\n",
    "    \"\"\"데이터셋의 이미지를 역정규화하여 격자 형태로 시각화합니다.\"\"\"\n",
    "    if start_index + num_images > len(dataset):\n",
    "        num_images = len(dataset) - start_index\n",
    "\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 4 * rows))\n",
    "    axes = np.atleast_1d(axes).flatten()\n",
    "\n",
    "    for i in range(num_images):\n",
    "        current_idx = start_index + i\n",
    "        image_tensor, label, _ = dataset[current_idx]\n",
    "\n",
    "        img_to_show = unnormalize(image_tensor)\n",
    "        \n",
    "        axes[i].imshow(img_to_show)\n",
    "        axes[i].set_title(f\"Idx: {current_idx} | Lbl: {label}\")\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    # 남은 빈 칸 제거\n",
    "    for j in range(num_images, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# show_dataset_grid(valid_dataset, start_index=0, num_images=500, cols=10)\n",
    "# show_dataset_grid(valid_dataset, start_index=501, num_images=500, cols=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d24a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epochs = 1\n",
    "step_epochs = 3\n",
    "\n",
    "models_list = []\n",
    "\n",
    "for epoch in range(start_epochs, max_epochs + 1, step_epochs):\n",
    "    model_path = f'./models/cam_model_pre_e{epoch}.pt'\n",
    "    \n",
    "    model = torch.load(model_path,  map_location=device, weights_only=False)\n",
    "    model.eval() \n",
    "    \n",
    "    models_list.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8fbc9b",
   "metadata": {},
   "source": [
    "### 단일 객체일 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbe773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label, ground_bbox = valid_dataset[0]\n",
    "item = image.unsqueeze(0).to(device)\n",
    "orig_img = unnormalize(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6cd715",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cam(models_list, image, ground_bbox, item, start_epochs, step_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7250d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_grad_cam(models_list, image, ground_bbox, item, start_epochs, step_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb953a2e",
   "metadata": {},
   "source": [
    "### 여러 객체가 있을 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c58a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label, ground_bbox = valid_dataset[326]\n",
    "item = image.unsqueeze(0).to(device)\n",
    "orig_img = unnormalize(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49de9992",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cam(models_list, image, ground_bbox, item, start_epochs, step_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf84a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_grad_cam(models_list, image, ground_bbox, item, start_epochs, step_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e0009b",
   "metadata": {},
   "source": [
    "### 객체가 멀리 있을 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72c62af",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label, ground_bbox = valid_dataset[544]\n",
    "# image, label, ground_bbox = valid_dataset[292]\n",
    "item = image.unsqueeze(0).to(device)\n",
    "orig_img = unnormalize(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a58cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cam(models_list, image, ground_bbox, item, start_epochs, step_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873d0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_grad_cam(models_list, image, ground_bbox, item, start_epochs, step_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c61c536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_center_distance(bbox1, bbox2):\n",
    "    \"\"\"두 Bounding Box의 중심점 간의 유클리드 거리를 계산합니다.\"\"\"\n",
    "    if bbox1 is None or bbox2 is None:\n",
    "        return float('inf')\n",
    "    \n",
    "    # 각 bbox의 중심점 계산\n",
    "    cx1 = (bbox1[0] + bbox1[2]) / 2.0\n",
    "    cy1 = (bbox1[1] + bbox1[3]) / 2.0\n",
    "    cx2 = (bbox2[0] + bbox2[2]) / 2.0\n",
    "    cy2 = (bbox2[1] + bbox2[3]) / 2.0\n",
    "    \n",
    "    # 유클리드 거리 반환\n",
    "    return math.sqrt((cx1 - cx2)**2 + (cy1 - cy2)**2)\n",
    "\n",
    "def evaluate_pointing_game(grad_cam_map, gt_bbox):\n",
    "    \"\"\"Grad-CAM 히트맵의 최대 활성화 지점이 정답 bbox 안에 있는지 평가합니다.\"\"\"\n",
    "    if gt_bbox is None:\n",
    "        return False, (0, 0)\n",
    "    \n",
    "    # 2D 히트맵에서 최댓값을 가지는 좌표(y, x) 추출\n",
    "    y_peak, x_peak = np.unravel_index(np.argmax(grad_cam_map), grad_cam_map.shape)\n",
    "    \n",
    "    x_min, y_min, x_max, y_max = gt_bbox\n",
    "    # 최대 활성화 지점이 정답 상자 내부에 있는지 확인 (Hit 판별)\n",
    "    is_hit = (x_min <= x_peak <= x_max) and (y_min <= y_peak <= y_max)\n",
    "    \n",
    "    return is_hit, (x_peak, y_peak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10ae82c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_grid_results(model, viz_image, ground_bbox, model_input, idx, modes=['overlay', 'iou', 'center_dist', 'pointing', 'combined']):\n",
    "    base_dir = \"img\"\n",
    "    # 전달받은 modes에 대해서만 폴더 생성\n",
    "    for mode in modes:\n",
    "        os.makedirs(os.path.join(base_dir, f\"grad_cam_{mode}\"), exist_ok=True)\n",
    "\n",
    "    target_layers = [name for name, module in model.named_modules() if isinstance(module, torch.nn.Conv2d)]\n",
    "    orig_img = unnormalize(viz_image)\n",
    "    ground_truth = [int(x) for x in ground_bbox]\n",
    "    \n",
    "    for mode in modes:\n",
    "        if mode == 'combined':\n",
    "            # 행(레이어 수) x 열(4가지 지표)\n",
    "            fig, axes = plt.subplots(len(target_layers), 4, figsize=(16, 3 * len(target_layers)))\n",
    "            \n",
    "            # 맨 위 열 제목 설정\n",
    "            col_titles = ['1. Overlay', '2. IoU', '3. Center Distance', '4. Pointing Game']\n",
    "            for c in range(4):\n",
    "                axes[0, c].set_title(col_titles[c], fontsize=14, fontweight='bold', pad=10)\n",
    "\n",
    "            for i, layer_name in enumerate(target_layers):\n",
    "                try:\n",
    "                    grad_cam = generate_grad_cam(model, model_input, activation_layer=layer_name)\n",
    "                    grad_cam_resized = cv2.resize(grad_cam, (orig_img.shape[1], orig_img.shape[0]))\n",
    "                    grad_cam_bbox = get_bbox(grad_cam_resized, threshold=0.5)\n",
    "                    \n",
    "                    # 1. Overlay (첫 번째 열)\n",
    "                    axes[i, 0].imshow(orig_img)\n",
    "                    axes[i, 0].imshow(grad_cam_resized, cmap='jet', alpha=0.5)\n",
    "                    axes[i, 0].set_ylabel(f\"{layer_name}\", fontsize=12, fontweight='bold') # 왼쪽에 레이어 이름 표시\n",
    "                    axes[i, 0].set_xticks([])\n",
    "                    axes[i, 0].set_yticks([])\n",
    "                    \n",
    "                    # 2. IoU (두 번째 열)\n",
    "                    iou_score = get_iou(grad_cam_bbox, ground_truth)\n",
    "                    res_img_iou = visualize_both_bbox_on_image(orig_img.copy(), grad_cam_bbox, ground_truth)\n",
    "                    axes[i, 1].imshow(res_img_iou)\n",
    "                    axes[i, 1].set_title(f\"IOU: {iou_score:.3f}\", fontsize=11)\n",
    "                    axes[i, 1].axis(\"off\")\n",
    "                    \n",
    "                    # 3. Center Distance (세 번째 열)\n",
    "                    dist = get_center_distance(grad_cam_bbox, ground_truth)\n",
    "                    res_img_dist = visualize_both_bbox_on_image(orig_img.copy(), grad_cam_bbox, ground_truth)\n",
    "                    axes[i, 2].imshow(res_img_dist)\n",
    "                    if grad_cam_bbox is not None:\n",
    "                        axes[i, 2].plot((grad_cam_bbox[0]+grad_cam_bbox[2])/2, (grad_cam_bbox[1]+grad_cam_bbox[3])/2, 'ro', markersize=6)\n",
    "                    axes[i, 2].plot((ground_truth[0]+ground_truth[2])/2, (ground_truth[1]+ground_truth[3])/2, 'go', markersize=6)\n",
    "                    axes[i, 2].set_title(f\"Dist: {dist:.1f}px\", fontsize=11)\n",
    "                    axes[i, 2].axis(\"off\")\n",
    "\n",
    "                    # 4. Pointing Game (네 번째 열)\n",
    "                    is_hit, (peak_x, peak_y) = evaluate_pointing_game(grad_cam_resized, ground_truth)\n",
    "                    hit_text, color = (\"HIT\", 'green') if is_hit else (\"MISS\", 'red')\n",
    "                    axes[i, 3].imshow(orig_img)\n",
    "                    rect = plt.Rectangle((ground_truth[0], ground_truth[1]), ground_truth[2]-ground_truth[0], ground_truth[3]-ground_truth[1], fill=False, edgecolor='green', linewidth=2)\n",
    "                    axes[i, 3].add_patch(rect)\n",
    "                    axes[i, 3].plot(peak_x, peak_y, marker='*', color=color, markersize=12)\n",
    "                    axes[i, 3].set_title(hit_text, fontsize=11, color=color, fontweight='bold')\n",
    "                    axes[i, 3].axis(\"off\")\n",
    "\n",
    "                except Exception:\n",
    "                    for c in range(4):\n",
    "                        axes[i, c].text(0.5, 0.5, \"Error\", ha='center')\n",
    "                        axes[i, c].axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            fig.subplots_adjust(top=0.97)\n",
    "            save_path = os.path.join(base_dir, f\"grad_cam_{mode}\", f\"{idx}.png\")\n",
    "            plt.savefig(save_path, bbox_inches='tight')\n",
    "            plt.close(fig)\n",
    "\n",
    "        else:\n",
    "            num_plots = len(target_layers) + 1\n",
    "            cols = 7\n",
    "            rows = math.ceil(num_plots / cols)\n",
    "            fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))\n",
    "            axes_flat = axes.flatten()\n",
    "\n",
    "            axes_flat[0].imshow(orig_img)\n",
    "            axes_flat[0].set_title(\"Original / GT\", fontsize=10)\n",
    "            axes_flat[0].axis(\"off\")\n",
    "\n",
    "            for i, layer_name in enumerate(target_layers):\n",
    "                ax = axes_flat[i + 1]\n",
    "                try:\n",
    "                    grad_cam = generate_grad_cam(model, model_input, activation_layer=layer_name)\n",
    "                    grad_cam_resized = cv2.resize(grad_cam, (orig_img.shape[1], orig_img.shape[0]))\n",
    "                    \n",
    "                    if mode == 'overlay':\n",
    "                        ax.imshow(orig_img)\n",
    "                        ax.imshow(grad_cam_resized, cmap='jet', alpha=0.5)\n",
    "                        ax.set_title(layer_name, fontsize=8)\n",
    "                    elif mode == 'iou':\n",
    "                        grad_cam_bbox = get_bbox(grad_cam_resized, threshold=0.5)\n",
    "                        iou_score = get_iou(grad_cam_bbox, ground_truth)\n",
    "                        res_img = visualize_both_bbox_on_image(orig_img.copy(), grad_cam_bbox, ground_truth)\n",
    "                        ax.imshow(res_img)\n",
    "                        ax.set_title(f\"{layer_name}\\nIOU: {iou_score:.3f}\", fontsize=8)\n",
    "                    elif mode == 'center_dist':\n",
    "                        grad_cam_bbox = get_bbox(grad_cam_resized, threshold=0.5)\n",
    "                        dist = get_center_distance(grad_cam_bbox, ground_truth)\n",
    "                        res_img = visualize_both_bbox_on_image(orig_img.copy(), grad_cam_bbox, ground_truth)\n",
    "                        ax.imshow(res_img)\n",
    "                        if grad_cam_bbox is not None:\n",
    "                            ax.plot((grad_cam_bbox[0]+grad_cam_bbox[2])/2, (grad_cam_bbox[1]+grad_cam_bbox[3])/2, 'ro', markersize=4)\n",
    "                        ax.plot((ground_truth[0]+ground_truth[2])/2, (ground_truth[1]+ground_truth[3])/2, 'go', markersize=4)\n",
    "                        ax.set_title(f\"{layer_name}\\nDist: {dist:.1f}px\", fontsize=8)\n",
    "                    elif mode == 'pointing':\n",
    "                        is_hit, (peak_x, peak_y) = evaluate_pointing_game(grad_cam_resized, ground_truth)\n",
    "                        hit_text = \"HIT\" if is_hit else \"MISS\"\n",
    "                        color = 'green' if is_hit else 'red'\n",
    "                        ax.imshow(orig_img)\n",
    "                        rect = plt.Rectangle((ground_truth[0], ground_truth[1]), ground_truth[2]-ground_truth[0], ground_truth[3]-ground_truth[1], fill=False, edgecolor='green', linewidth=2)\n",
    "                        ax.add_patch(rect)\n",
    "                        ax.plot(peak_x, peak_y, marker='*', color=color, markersize=10)\n",
    "                        ax.set_title(f\"{layer_name}\\n{hit_text}\", fontsize=8)\n",
    "                        \n",
    "                except Exception:\n",
    "                    ax.text(0.5, 0.5, \"Error\", ha='center')\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "            for j in range(num_plots, len(axes_flat)):\n",
    "                axes_flat[j].axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            fig.subplots_adjust(top=0.97)\n",
    "            \n",
    "            save_path = os.path.join(base_dir, f\"grad_cam_{mode}\", f\"{idx}.png\")\n",
    "            plt.savefig(save_path, bbox_inches='tight')\n",
    "            plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca9baa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './models/cam_model_pre_e3.pt'\n",
    "    \n",
    "model = torch.load(model_path,  map_location=device, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725006da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 설정 모드 선택 ---\n",
    "execution_mode = 'range' \n",
    "# 수행할 분석 지표를 리스트로 전달합니다. 필요한 것만 남기면 됩니다.\n",
    "selected_modes = ['overlay', 'iou', 'center_dist', 'pointing', 'combined']\n",
    "\n",
    "# 만약 4가지가 합쳐진 'combined' 파일 1개만 딱 뽑고 싶다면 이렇게 설정합니다.\n",
    "# selected_modes = ['combined']\n",
    "\n",
    "# --- 2. 모드별 세부 설정 ---\n",
    "start_idx, end_idx = 540, 600  \n",
    "num_random_samples = 50        \n",
    "\n",
    "all_indices = list(range(len(valid_dataset)))\n",
    "\n",
    "if execution_mode == 'all':\n",
    "    indices_to_process = all_indices\n",
    "    print(f\"전체 데이터({len(indices_to_process)}개)를 처리합니다.\")\n",
    "\n",
    "elif execution_mode == 'range':\n",
    "    indices_to_process = all_indices[max(0, start_idx) : min(len(all_indices), end_idx)]\n",
    "    print(f\"{start_idx}번부터 {end_idx-1}번까지 {len(indices_to_process)}개의 데이터를 처리합니다.\")\n",
    "\n",
    "elif execution_mode == 'random':\n",
    "    indices_to_process = random.sample(all_indices, min(num_random_samples, len(all_indices)))\n",
    "    print(f\"무작위로 {len(indices_to_process)}개의 데이터를 추출하여 처리합니다: {indices_to_process}\")\n",
    "\n",
    "# --- 3. 모델 준비 및 루프 실행 ---\n",
    "device = next(model.parameters()).device\n",
    "model.eval()\n",
    "\n",
    "for idx in tqdm(indices_to_process, desc=f\"Mode: {execution_mode}\"):\n",
    "    image, label, ground_bbox = valid_dataset[idx]\n",
    "    item = image.unsqueeze(0).to(device)\n",
    "    save_grid_results(model, image, ground_bbox, item, idx, modes=selected_modes)\n",
    "    \n",
    "print(\"작업이 완료되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2a9584",
   "metadata": {},
   "source": [
    "### 이미지를 출력한 상태로 파일을 업로드시 노트북 용량이 너무 크기 때문에 변경\n",
    "### 1.  단일객체\n",
    "[CAM 단일 객체](./img/cam_1.png)  \n",
    "[Grad CAM 단일 객체](./img/grad_cam_1.png)  \n",
    "\n",
    "### 2. 여러 객체\n",
    "[CAM 여러 객체](./img/cam_2.png)  \n",
    "[Grad 여러 단일 객체](./img/grad_cam_2.png)\n",
    "\n",
    "### 3. 먼 거리 객체\n",
    "[CAM 먼 거리 객체](./img/cam_3.png)  \n",
    "[Grad CAM 먼 거리 객체](./img/grad_cam_3.png)\n",
    "\n",
    "### IoU가 지역성과 모델의 성능을 판단하기 위한 절대적인 지표가 될 수 있는가?\n",
    "1. IoU는 모델이 \"물체가 어디에 있는지\"를 얼마나 잘 찾아냈는지(지역성)를 평가하기에는 훌륭한 지표입니다. 하지만 IoU가 높다고 해서 반드시 '성능이 좋은 모델'이라고 단정할 수는 없습니다.\n",
    "\n",
    "    - 이유: 모델의 최종 목적이 '분류'라면, 객체의 전체 영역을 대충 훑는 것보다 핵심적인 특징(Discriminative parts)을 정확히 짚어내는 것이 분류 정확도를 높이는 데 유리하기 때문입니다.\n",
    "\n",
    "\n",
    "2. Grad-CAM 레이어 비교 분석\n",
    "이미지 상의 Ep 4 결과를 바탕으로 비교해 보겠습니다.\n",
    "\n",
    "- 레이어 3 (IoU: 0.6809):   \n",
    "    특징: 히트맵이 치와와의 몸 전체와 얼굴을 비교적 넓게 커버하고 있습니다.  \n",
    "    평가: 객체의 기하학적 형태를 잘 반영하고 있어 지역성 지표(IoU)는 높게 나타납니다. 하지만 분류를 위한 핵심 특징 외에 배경이나 덜 중요한 부위까지 포함하고 있을 가능성이 큽니다.\n",
    "\n",
    "- 레이어 4 (IoU: 0.1623):  \n",
    "    특징: 히트맵이 치와와의 눈과 코 등 '얼굴' 부위에만 매우 집중(Focus)되어 있습니다.  \n",
    "    평가: 객체 전체를 덮지 못해 IoU는 낮게 측정됩니다. 그러나 모델이 이 동물을 '치와와'로 판단하게 만든 결정적인 근거는 바로 이 얼굴 부위입니다. 즉, 분류를 위한 추상화 수준은 더 높다고 볼 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be7960",
   "metadata": {},
   "source": [
    "[참고 자료](./img/grad_cam_alternative_544.png) \n",
    "\n",
    "### IoU가 절대적인 지표가 될 수 없다면 어떤 지표를 사용하면 되는가? \n",
    "\n",
    "- DIoU (Distance): 두 박스의 중심점 거리를 직접 계산에 포함해 수렴 속도를 높임.\n",
    "\n",
    "- Point Detection: 중심점이나 특정 핵심 포인트가 정답 영역 내에 위치하는지를 판별하여 박스 전체의 겹침 정도와 무관하게 타겟 포착 여부를 직관적으로 측정함\n",
    "\n",
    "- GIoU (Generalized): 겹치지 않아도 두 박스를 포함하는 최소 크기의 박스를 이용해 거리를 계산함.\n",
    "\n",
    "- CIoU (Complete): 중첩 영역, 중심점 거리, 그리고 종횡비까지 모두 고려한 끝판왕 지표."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
